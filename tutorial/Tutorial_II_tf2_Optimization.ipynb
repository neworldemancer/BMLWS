{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Tutorial_II_tf2_Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YLKYlLJRgW91",
        "EqADCB3D9Rxo",
        "DInAkgl1gW93",
        "HJaPvpOFgW98",
        "XpJ1CBW2gW99",
        "UtXcbsUJgW-I",
        "gpxvwQgKgW-L"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "berwLM3NgW9v"
      },
      "source": [
        "# Tutorial II: Optimization in TensorFlow & NN introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3q4heOGgW9x"
      },
      "source": [
        "<p>\n",
        "Bern Winter School on Machine Learning, 2022<br>\n",
        "Prepared by Mykhailo Vladymyrov.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLKYlLJRgW91"
      },
      "source": [
        "## 1. Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJvQBwIoUb0Q"
      },
      "source": [
        "colab = True # set to True is using google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4bwmIrggalE"
      },
      "source": [
        "if colab:\n",
        "    %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjwECu5bgW92"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqADCB3D9Rxo"
      },
      "source": [
        "### Download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMxFBHhV9Rxq"
      },
      "source": [
        "if colab:\n",
        "    path = os.path.abspath('.')+'/material.tgz'\n",
        "    url = 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz'\n",
        "    p = tf.keras.utils.get_file(path, url)\n",
        "    assert p==path\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cInCJilaOC5A"
      },
      "source": [
        "from utils import gr_disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdI07bJXUb0j"
      },
      "source": [
        "def show_graph(g=None, gd=None):\n",
        "    gr_disp.show_graph(g, gd)\n",
        "    %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DInAkgl1gW93"
      },
      "source": [
        "## 2. Linear fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ELO5INFgW94"
      },
      "source": [
        "Here we will solve optimization problem to perform linear regression. First we will generate training set of 80 data points and test set of 20, laying on a line with a random offset $$y = a_0 x + b_0 + \\delta,$$ where $\\delta$ is a random variable sampled from a uniform distribution with standard deviation equal to $s_0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h6vmx-7gW94"
      },
      "source": [
        "a0 = 3  # 3\n",
        "b0 = 5  # 5\n",
        "s0 = 1  # 1\n",
        "\n",
        "# all samples\n",
        "x_all = np.linspace(0, 10, 100)  # 100 points\n",
        "n_all = x_all.shape[0]\n",
        "d_all = np.random.uniform(-s0, s0, size=n_all)\n",
        "y_all = np.asarray([a0*x + b0 + d for x, d in zip(x_all, d_all)])\n",
        "    \n",
        "# randomize order and get 80% for training\n",
        "idx = np.random.permutation(n_all)\n",
        "n_train = n_all * 80 // 100\n",
        "\n",
        "idx_train = idx[0:n_train]\n",
        "idx_val = idx[n_train:]\n",
        "\n",
        "x_train = x_all[idx_train]\n",
        "y_train = y_all[idx_train]\n",
        "\n",
        "x_val = x_all[idx_val]\n",
        "y_val = y_all[idx_val]\n",
        "\n",
        "plt.plot(x_train, y_train, \"or\", x_val, y_val, \"b^\")\n",
        "plt.legend(('training points', 'validation points'),  loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sERniBfcgW96"
      },
      "source": [
        "We will then define loss function as the mean of squared residuals (distance from line along $y$) for the points.\n",
        "\n",
        "We will use [stochactic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): on each iteration use only a fraction (`batch_size`) of all training set. In many cases training set is huge and cannot be fed on each iteration in principle. Also it can sometimes help the optimizer to properly explore the manifold."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.a = self.add_weight(initializer=tf.keras.initializers.random_normal(),\n",
        "                             name='a', dtype=tf.float32)\n",
        "    self.b = self.add_weight(initializer=tf.keras.initializers.random_normal(),\n",
        "                             name='a', dtype=tf.float32)\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    f = self.a * x + self.b\n",
        "    return f\n",
        "\n",
        "def loss_f(true_y, y_predicted):\n",
        "  residual = true_y - y_predicted\n",
        "  squared_residual = residual ** 2\n",
        "  mean_squared_residual = tf.reduce_mean(squared_residual)\n",
        "  return mean_squared_residual\n",
        "    \n",
        "x = tf.keras.layers.Input(name='x', dtype=tf.float32, shape=())\n",
        "y = Linear()(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=x, outputs=y)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.002)\n",
        "model.compile(optimizer=optimizer, loss=loss_f)  # we could just use `loss='mse'` - it does the very same thing\n",
        "model.summary()\n",
        "\n",
        "\n",
        "batch_size = 10 # 10\n",
        "\n",
        "hist = model.fit(x=x_train, y=y_train, \n",
        "                 batch_size=batch_size,\n",
        "                 epochs=1000, shuffle=True,\n",
        "                 validation_data=(x_val, y_val), \n",
        "                 )"
      ],
      "metadata": {
        "id": "MLUSA-htxALJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_val, b_val = model.get_weights()\n",
        "        \n",
        "end_fit_x = [x_all[0], x_all[-1]]\n",
        "end_fit_y = [a_val*x+b_val for x in end_fit_x]\n",
        "true_fn_y = [a0*x+b0 for x in end_fit_x]\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10,8))\n",
        "axs[0].plot(x_train, y_train, 'ro', x_val, y_val, 'b^', end_fit_x, end_fit_y, 'g')\n",
        "axs[0].legend(('training points', 'test points', 'final fit'),  loc='upper left')\n",
        "\n",
        "ep_arr = hist.epoch\n",
        "axs[1].semilogy(ep_arr, hist.history['loss'], 'r')\n",
        "axs[1].semilogy(ep_arr, hist.history['val_loss'], 'b')\n",
        "axs[1].legend(('training loss', 'test loss'),  loc='upper right');"
      ],
      "metadata": {
        "id": "Mr0urzvu0CoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJaPvpOFgW98"
      },
      "source": [
        "## 3. Excercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfFwDCNEgW99"
      },
      "source": [
        "Play with the true function parameters ```a0, b0, s0``` and the ``batch_size`` value, check how it affects the convergence.\n",
        "\n",
        "1. How change of `s0` affects convergance?\n",
        "2. When one should stop training to prevent overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpJ1CBW2gW99"
      },
      "source": [
        "## 4. A bit of things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJUUnwWcgW9-"
      },
      "source": [
        "The training as we just saw is done iteratively, by adjusting the model parameters.\n",
        "\n",
        "We perform optimization several times for all traininng dataset. Going through all this dataset is refered to as 'epoch'.\n",
        "\n",
        "When we do training its usually done in two loops. In outer loop we iterate over all epochs. For each epoch we usually split the dataset into small chuncks, batches, and optimization it performed for all of those.\n",
        "\n",
        "It is important that data doesn't go to the training pipeline in same order. So the overall scheme looks like this (pseudocode):\n",
        "\n",
        "\n",
        "```\n",
        "x,y = get_training_data()\n",
        "for epoch in range(number_epochs):\n",
        "   x_shfl,y_shfl = shuffle(x,y)\n",
        "   \n",
        "   for mb_idx in range(number_minibatches_in_batch):\n",
        "       x_mb,y_mb = get_minibatch(x_shfl,y_shfl, mb_idx)\n",
        "       \n",
        "       optimize_on(data=x_mb, labels=y_mb)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtXcbsUJgW-I"
      },
      "source": [
        "## 5. Bulding blocks of a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ1JOmpCgW-I"
      },
      "source": [
        "Neural network consists of layers of neurons. Each neuron perfroms 2 operations.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/Perceptron.png\" alt=\"drawing\" width=\"30%\"/>\n",
        "\n",
        "1. Calculate the linear transformation of the input vector $\\bar x$: \n",
        "$$z = \\bar W \\cdot \\bar x + b = \\sum {W_i x_i} + b$$ where $\\bar W$ is vector of weights and $b$ - bias.\n",
        "2. Perform the nonlinear transformation of the result using activation function $f$ $$y = f(z)$$ Here we will use rectified linear unit activation.\n",
        "\n",
        "In a fully connected neural network each layer is a set of N neurons, performing different transformations of all the same layer's inputs $\\bar x = [x_i]$ producing output vector $\\bar y = [y_j]_{i=1..N}$: $$y_j = f(\\bar W_j \\cdot \\bar x + b_j)$$\n",
        "\n",
        "Since output of each layer forms input of next layer, one can write for layer $l$: $$x^l_j = f(\\bar W^l_j \\cdot \\bar x^{l-1} + b^l_j)$$ where $\\bar x^0$ is network's input vactor.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/MLP.png\" alt=\"drawing\" width=\"50%\"/>\n",
        "\n",
        "To simplify building the network, we'll define a helper function, creating neuron layer with given number of outputs:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.keras.layers.Dense??"
      ],
      "metadata": {
        "id": "gg8X9qOB3MC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "iaU1t0WngW-J"
      },
      "source": [
        "class Dense(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               units,\n",
        "               activation=None,\n",
        "               ):\n",
        "    \"\"\"Fully connected layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    units : int\n",
        "        Number of output neurons\n",
        "    name : None, optional\n",
        "        TF Scope to apply\n",
        "    activation : None, optional\n",
        "        Non-linear activation function\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    h, W : tf.Tensor, tf.Tensor\n",
        "        Output of the fully connected layer and the weight matrix\n",
        "    \"\"\"\n",
        "    super(self).__init__(**kwargs)\n",
        "    self.units = int(units)\n",
        "    self.activation = activations.get(activation)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = tf.TensorShape(input_shape)\n",
        "    last_dim = tf.compat.dimension_value(input_shape[-1])\n",
        "\n",
        "    self.kernel = self.add_weight(\n",
        "        'kernel',\n",
        "        shape=[last_dim, self.units],\n",
        "        initializer=self.kernel_initializer,\n",
        "        dtype=self.dtype,\n",
        "        trainable=True)\n",
        "    \n",
        "\n",
        "    self.bias = self.add_weight(\n",
        "          'bias',\n",
        "          shape=[self.units,],\n",
        "          initializer=self.bias_initializer,\n",
        "          dtype=self.dtype,\n",
        "          trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = tf.matmul(a=inputs, b=self.kernel)\n",
        "    outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "\n",
        "    if self.activation is not None:\n",
        "      outputs = self.activation(outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7gzOyONgW-K"
      },
      "source": [
        "In the case of classification, in the the last layer we use *softmax* transformation as non-linear transformation: $$y_i = \\sigma(\\bar z)_i = \\frac{ e^{z_i}}{\\sum_j e^{z_j}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyRaNDlVgW-L"
      },
      "source": [
        "This will correspond to the one-hot labels that we use.\n",
        "Finally we will use the cross entropy between output $y$ and the ground truth (GT) $y_{GT}$ as the loss function: $$H(y, y_{GT}) = - \\sum_i y_{GT, i} \\log(y_{i})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxvwQgKgW-L"
      },
      "source": [
        "## 6. Bulding a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba_cMJuJgW-M"
      },
      "source": [
        "n_input = 10\n",
        "n_output = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqlSKoY8gW-O"
      },
      "source": [
        "x = tf.keras.layers.Input(name='X', dtype=tf.float32, shape=[n_input])\n",
        "\n",
        "#layer 1: 10 inputs -> 4, sigmoid activation\n",
        "l1 = tf.keras.layers.Dense(units=4, name='L1', activation='sigmoid')(x)\n",
        "\n",
        "#layer 2: 4 inputs -> 2, softmax activation\n",
        "l2 = tf.keras.layers.Dense(units=n_output, name='L2', activation='softmax')(l1)\n",
        "   \n",
        "#prediction: onehot->integer\n",
        "pred = tf.argmax(l2, axis=1, name='pred')\n",
        "\n",
        "model = tf.keras.Model(inputs=x, outputs=[l2, pred])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwgSoCjQgW-P"
      },
      "source": [
        "print(x)\n",
        "print(l1)\n",
        "print(l2)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cINev65WgW-R"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5qoNhQXUEKC"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/geekculture/how-visualizations-help-make-sense-of-deep-learning-a408ab00688f"
      ],
      "metadata": {
        "id": "ZsGHp1_fZBbV"
      }
    }
  ]
}