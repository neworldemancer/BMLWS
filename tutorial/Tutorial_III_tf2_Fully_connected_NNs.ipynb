{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Tutorial_III_tf2_Fully_connected_NNs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dit404ghtkSr",
        "K2DojogoQv4l",
        "rcuT4JYRtkS4",
        "YzpETsFotkTF",
        "agJ3gkg0tkTJ",
        "cgHlUGwDtkTW",
        "hh6I4wJLtkTa",
        "kcLUPxNwtkTd",
        "TE_HXLPjfRBt",
        "I3v3Qp2WfMoE"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxDZW841tkSi"
      },
      "source": [
        "# Tutorial III: Handwritten digit recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nha4NG_tkSl"
      },
      "source": [
        "<p>\n",
        "Bern Winter School on Machine Learning, 2022<br>\n",
        "Prepared by Mykhailo Vladymyrov.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjuW0SKStkSm"
      },
      "source": [
        "In this session we will create a fully-connected neural network to perform handwritten digit recognition using TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dit404ghtkSr"
      },
      "source": [
        "## 1. Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E7_B0MiQv4a"
      },
      "source": [
        "colab = True # set to True is using google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fcfiHcmuAPA"
      },
      "source": [
        "if colab:\n",
        "    %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbRY_dIutkSr"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.datasets.mnist as mnist\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2DojogoQv4l"
      },
      "source": [
        "### Download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZokqZcQWgCj0"
      },
      "source": [
        "if colab:\n",
        "    path = os.path.abspath('.')+'/material.tgz'\n",
        "    url = 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz'\n",
        "    p = tf.keras.utils.get_file(path, url)\n",
        "    assert p==path\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8RkDsYJQv4o"
      },
      "source": [
        "from utils import gr_disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohH_OvIXQv4r"
      },
      "source": [
        "def show_graph(g=None, gd=None):\n",
        "    gr_disp.show_graph(g, gd)\n",
        "    %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpJ1CBW2gW99"
      },
      "source": [
        "## 2. Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJUUnwWcgW9-"
      },
      "source": [
        "The training as we just saw is done iteratively, by adjusting the model parameters.\n",
        "\n",
        "We perform optimization several times for all traininng dataset. Going through all this dataset is refered to as 'epoch'.\n",
        "\n",
        "When we do training its usually done in two loops. In outer loop we iterate over all epochs. For each epoch we usually split the dataset into small chuncks, batches, and optimization it performed for all of those.\n",
        "\n",
        "It is important that data doesn't go to the training pipeline in same order. So the overall scheme looks like this (pseudocode):\n",
        "\n",
        "\n",
        "```\n",
        "x,y = get_training_data()\n",
        "for epoch in range(number_epochs):\n",
        "   x_shfl,y_shfl = shuffle(x,y)\n",
        "   \n",
        "   for mb_idx in range(number_minibatches_in_batch):\n",
        "       x_mb,y_mb = get_minibatch(x_shfl,y_shfl, mb_idx)\n",
        "       \n",
        "       optimize_on(data=x_mb, labels=y_mb)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtXcbsUJgW-I"
      },
      "source": [
        "## 3. Bulding blocks of a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ1JOmpCgW-I"
      },
      "source": [
        "Neural network consists of layers of neurons. Each neuron perfroms 2 operations.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/Perceptron.png\" alt=\"drawing\" width=\"30%\"/>\n",
        "\n",
        "1. Calculate the linear transformation of the input vector $\\bar x$: \n",
        "$$z = \\bar W \\cdot \\bar x + b = \\sum {W_i x_i} + b$$ where $\\bar W$ is vector of weights and $b$ - bias.\n",
        "2. Perform the nonlinear transformation of the result using activation function $f$ $$y = f(z)$$ Here we will use rectified linear unit activation.\n",
        "\n",
        "In a fully connected neural network each layer is a set of N neurons, performing different transformations of all the same layer's inputs $\\bar x = [x_i]$ producing output vector $\\bar y = [y_j]_{i=1..N}$: $$y_j = f(\\bar W_j \\cdot \\bar x + b_j)$$\n",
        "\n",
        "Since output of each layer forms input of next layer, one can write for layer $l$: $$x^l_j = f(\\bar W^l_j \\cdot \\bar x^{l-1} + b^l_j)$$ where $\\bar x^0$ is network's input vactor.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/MLP.png\" alt=\"drawing\" width=\"50%\"/>\n",
        "\n",
        "To simplify building the network, we'll define a helper function, creating neuron layer with given number of outputs:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.keras.layers.Dense??"
      ],
      "metadata": {
        "id": "gg8X9qOB3MC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "iaU1t0WngW-J"
      },
      "source": [
        "class Dense(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               units,\n",
        "               activation=None,\n",
        "               ):\n",
        "    \"\"\"Fully connected layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    units : int\n",
        "        Number of output neurons\n",
        "    name : None, optional\n",
        "        TF Scope to apply\n",
        "    activation : None, optional\n",
        "        Non-linear activation function\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    h, W : tf.Tensor, tf.Tensor\n",
        "        Output of the fully connected layer and the weight matrix\n",
        "    \"\"\"\n",
        "    super(self).__init__(**kwargs)\n",
        "    self.units = int(units)\n",
        "    self.activation = activations.get(activation)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = tf.TensorShape(input_shape)\n",
        "    last_dim = tf.compat.dimension_value(input_shape[-1])\n",
        "\n",
        "    self.kernel = self.add_weight(\n",
        "        'kernel',\n",
        "        shape=[last_dim, self.units],\n",
        "        initializer=self.kernel_initializer,\n",
        "        dtype=self.dtype,\n",
        "        trainable=True)\n",
        "    \n",
        "\n",
        "    self.bias = self.add_weight(\n",
        "          'bias',\n",
        "          shape=[self.units,],\n",
        "          initializer=self.bias_initializer,\n",
        "          dtype=self.dtype,\n",
        "          trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = tf.matmul(a=inputs, b=self.kernel)\n",
        "    outputs = tf.nn.bias_add(outputs, self.bias)\n",
        "\n",
        "    if self.activation is not None:\n",
        "      outputs = self.activation(outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7gzOyONgW-K"
      },
      "source": [
        "In the case of classification, in the the last layer we use *softmax* transformation as non-linear transformation: $$y_i = \\sigma(\\bar z)_i = \\frac{ e^{z_i}}{\\sum_j e^{z_j}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyRaNDlVgW-L"
      },
      "source": [
        "This will correspond to the one-hot labels that we use.\n",
        "Finally we will use the cross entropy between output $y$ and the ground truth (GT) $y_{GT}$ as the loss function: $$H(y, y_{GT}) = - \\sum_i y_{GT, i} \\log(y_{i})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxvwQgKgW-L"
      },
      "source": [
        "## 4. Structure of a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba_cMJuJgW-M"
      },
      "source": [
        "n_input = 10\n",
        "n_output = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqlSKoY8gW-O"
      },
      "source": [
        "x = tf.keras.layers.Input(name='X', dtype=tf.float32, shape=[n_input])\n",
        "\n",
        "#layer 1: 10 inputs -> 4, sigmoid activation\n",
        "l1 = tf.keras.layers.Dense(units=4, name='L1', activation='sigmoid')(x)\n",
        "\n",
        "#layer 2: 4 inputs -> 2, softmax activation\n",
        "l2 = tf.keras.layers.Dense(units=n_output, name='L2', activation='softmax')(l1)\n",
        "   \n",
        "#prediction: onehot->integer\n",
        "pred = tf.argmax(l2, axis=1, name='pred')\n",
        "\n",
        "model = tf.keras.Model(inputs=x, outputs=[l2, pred])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwgSoCjQgW-P"
      },
      "source": [
        "print(x)\n",
        "print(l1)\n",
        "print(l2)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cINev65WgW-R"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5qoNhQXUEKC"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcuT4JYRtkS4"
      },
      "source": [
        "## 5. Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrbUAUlrtkS4"
      },
      "source": [
        "First we will load the data: 60000 training images and 10000 images for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L33vRL03tkS7"
      },
      "source": [
        "(x_train_2d, y_train), (x_test_2d, y_test) = mnist.load_data()\n",
        "x_train_2d = x_train_2d/255.0\n",
        "x_test_2d = x_test_2d/255.0\n",
        "\n",
        "\n",
        "print ('train: data shape', x_train_2d.shape, 'label shape', y_train.shape)\n",
        "print ('test: data shape', x_test_2d.shape, 'label shape', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1eXOuZOtkS8"
      },
      "source": [
        "Each image is a 28x28 pixels. For this model we will interppret it as a 1D array of 784 elements.\n",
        "Additionally we will use the labels a in so-called one hot encoding: each label is a vector of length 10, with all elements equal to 0, except, corresponding to the number written in the image. Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JouvebGFSzj"
      },
      "source": [
        "n_train = x_train_2d.shape[0]\n",
        "n_test = x_test_2d.shape[0]\n",
        "\n",
        "x_train = x_train_2d.reshape([n_train, -1])\n",
        "x_test = x_test_2d.reshape([n_test, -1])\n",
        "\n",
        "y_train_1_hot = np.zeros((n_train, y_train.max()+1))\n",
        "y_train_1_hot[np.arange(n_train),y_train] = 1\n",
        "\n",
        "y_test_1_hot = np.zeros((n_test, y_test.max()+1))\n",
        "y_test_1_hot[np.arange(n_test), y_test] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRO6zv6atkS_"
      },
      "source": [
        "img = x_train_2d[0]\n",
        "lbl = y_train[0]\n",
        "lbl_1_hot = y_train_1_hot[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBbmxF94p8Ve"
      },
      "source": [
        "Let's check the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrN9qvs3tkTA"
      },
      "source": [
        "plt.imshow(img, cmap='gray', interpolation='nearest')\n",
        "plt.grid(False)\n",
        "print('one-hot label:',lbl_1_hot, '. Actual label:', lbl )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d-oTmh3tkTE"
      },
      "source": [
        "fig, axs = plt.subplots(5, 5, figsize=(10,10))\n",
        "for idx, im in enumerate(x_train_2d[0:25]):\n",
        "    y_idx = idx // 5\n",
        "    x_idx = idx % 5\n",
        "    axs[y_idx][x_idx].imshow(im, cmap='gray', interpolation='nearest')\n",
        "    axs[y_idx][x_idx].grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agJ3gkg0tkTJ"
      },
      "source": [
        "## 6. Bulding a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfJEnLTmtkTK"
      },
      "source": [
        "Number of inputs for neurons will be given by input data, i.e. image, size. Output - by number of classes, 10 in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC1SpGLbtkTL"
      },
      "source": [
        "n_input = x_train.shape[1]\n",
        "n_output = y_train_1_hot.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7SF4JZgtkTP"
      },
      "source": [
        "Let's first build a most simple network, with just 1 layer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.layers.Input(name='X', dtype=tf.float32, shape=[n_input])\n",
        "\n",
        "# 1 layer: 768 inputs -> 10 outputs, softmax activation\n",
        "l1 = tf.keras.layers.Dense(units=n_output, name='L1', activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=x, outputs=l1)"
      ],
      "metadata": {
        "id": "r5MqC-QJAIK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fBowak_lAUEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFwr5MLMjxI1"
      },
      "source": [
        "Model summary provides information about the model's layers and trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCttp5zeb5l2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HIikZnSJhzT"
      },
      "source": [
        "We will train for 5 epochs, with batches of size 64. This is very similar to what we did in last session: split all data in batches, run the optimizer, for each epoch store the training and validadtion  accuracy for plotting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x=x_train, y=y_train_1_hot,\n",
        "                 epochs=5, batch_size=64,\n",
        "                 validation_data=(x_test, y_test_1_hot), \n",
        "                 )"
      ],
      "metadata": {
        "id": "kLidmnCaBN64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "axs[0].plot(hist.epoch, hist.history['loss'])\n",
        "axs[0].plot(hist.epoch, hist.history['val_loss'])\n",
        "axs[0].legend(('training loss', 'validation loss'), loc='lower right')\n",
        "axs[1].plot(hist.epoch, hist.history['accuracy'])\n",
        "axs[1].plot(hist.epoch, hist.history['val_accuracy'])\n",
        "\n",
        "axs[1].legend(('training accuracy', 'validation accuracy'), loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zKV09-n0Di1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get index of first incorrectly recognize digit and display it\n",
        "\n",
        "test_pred = model.predict(x_test)\n",
        "pred_class_idx = np.argmax(test_pred, axis=1)\n",
        "\n",
        "correct = pred_class_idx == y_test\n",
        "\n",
        "wrong_idx  = [i for i,c in enumerate(correct) if c == False]\n"
      ],
      "metadata": {
        "id": "KXSmBEU4DrYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_sample_idx = 5\n",
        "sample_idx = wrong_idx[wrong_sample_idx]\n",
        "\n",
        "plt.imshow(x_test_2d[sample_idx], cmap='gray', interpolation='nearest')\n",
        "plt.show()\n",
        "\n",
        "print(f'class: {y_test[sample_idx]}, predicted class:{pred_class_idx[sample_idx]}')"
      ],
      "metadata": {
        "id": "3Qmo5eCpEc89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1, b1 = model.get_weights()"
      ],
      "metadata": {
        "id": "p88CAkv2Facy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYoROCLrBQW"
      },
      "source": [
        "The learned model parameters W1 are a matrix of weights that show importance of each input pixel (784) for each of the 10 outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-nnCph8rU01"
      },
      "source": [
        "print(w1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i02IbJ2gtkTT"
      },
      "source": [
        "Let's visualize the trained weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L371n9COtkTU"
      },
      "source": [
        "w1 = w1.reshape(28,28,10)\n",
        "_, axs = plt.subplots(1, 10, figsize=(13,5))\n",
        "for i in range(10):\n",
        "    axs[i].imshow(w1[..., i], cmap='plasma', interpolation='nearest')\n",
        "    axs[i].grid(False)\n",
        "    axs[i].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuo23m_ytkTW"
      },
      "source": [
        "Here we classify images into 10 classes. But think of it: does the network know, or need to know that those were images? For the network each image is just a 784 values. And it finds that there is a patten in those!\n",
        "\n",
        "Same way one can feed any other bunch of numbers, and the network will try it's best to fugure out a relation pannern between those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgHlUGwDtkTW"
      },
      "source": [
        "## 7. Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4IKSsj9tkTX"
      },
      "source": [
        "Build a network with two layers, first with `tf.nn.relu` ReLU activation and 1500 neurons and second one with 10 and softmax activation. Start with `learning_rate` of 0.001 and find optimal value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKx1wnrRtkTX"
      },
      "source": [
        "???"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4V1-wkRtkTZ",
        "scrolled": false
      },
      "source": [
        "???"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh6I4wJLtkTa"
      },
      "source": [
        "## 8. Gradients visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzbUO_kZF9kI"
      },
      "source": [
        "idx = 111\n",
        "inp_v = x_train[idx:idx+1]  # use some image to compute gradients with respect to\n",
        "img = x_train_2d[idx]\n",
        "\n",
        "inp = tf.constant(inp_v)  # create tf constant tensor\n",
        "with tf.GradientTape() as tape:  # gradient tape for gradint evaluation\n",
        "  tape.watch(inp)  # take inp as variable\n",
        "  preds = model(inp) # evaluate model output\n",
        "\n",
        "grads = tape.jacobian(preds, inp)  # evaluate d preds[i] / d inp[j]\n",
        "grads = grads.numpy().reshape(list(grads.shape)[:-1] + [28, 28])\n",
        "print(grads.shape, '<- (Batch_preds, preds[i], Batch_inp, inp[y], inp[x])')\n",
        "grads = grads[0,:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ujfuAkVtkTb"
      },
      "source": [
        "We will display several images, and corresponding gradients of maximal output activation, as well as all activations. This might help better understand how our network processes the imput data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('prediction:', np.argmax(preds[0]))\n",
        "fig, axs = plt.subplots(1, 11, figsize=(4.1*11,4))\n",
        "axs[0].imshow(img)\n",
        "axs[0].set_title('raw')\n",
        "vmin,vmax = grads.min(), grads.max()\n",
        "for i, g in enumerate(grads):\n",
        "  axs[i+1].imshow(g, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "  axs[i+1].set_title(r'$\\frac{\\partial\\;P(digit\\,%d)}{\\partial\\;input}$' % i, fontdict={'size':16})"
      ],
      "metadata": {
        "id": "yr1XlRNVGFey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/geekculture/how-visualizations-help-make-sense-of-deep-learning-a408ab00688f"
      ],
      "metadata": {
        "id": "ZsGHp1_fZBbV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcLUPxNwtkTd"
      },
      "source": [
        "## 9. Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group 1"
      ],
      "metadata": {
        "id": "TE_HXLPjfRBt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BWuZAD3tkTf"
      },
      "source": [
        "Build the network with 3 or more layers. Try to get test accuracy >98.5%.\n",
        "Better to copy and modify the previous code than modyfy that one: then you can compare results.\n",
        "Visualize your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCJ1i_botkTg"
      },
      "source": [
        "g = tf.Graph()\n",
        "......\n",
        ".....\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group 2"
      ],
      "metadata": {
        "id": "I3v3Qp2WfMoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can repeat the same for a regression problem: here you will have just 1 output in the last layer, with no activation - to predict continuous unboud range. You can use the `mse` or `mae` loss. Compare results with a baseline linear / random forest model.\n",
        "\n",
        "How many parameters does your model have as compared to number of samples?"
      ],
      "metadata": {
        "id": "saHOeppBfaDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "x_train_mean = np.mean(x_train, axis=0)\n",
        "x_train_std = np.std(x_train, axis=0)\n",
        "\n",
        "x_train = (x_train - x_train_mean) / x_train_std\n",
        "x_test = (x_test - x_train_mean) / x_train_std"
      ],
      "metadata": {
        "id": "SukdjER1gYfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "Rb8xwJUMgzy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "U2kpPnnhg9n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y_test);"
      ],
      "metadata": {
        "id": "LfPTV8SVhPgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}