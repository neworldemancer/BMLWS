{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Tutorial_IV_tf2_Convolutions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l7s9QvYetmcY",
        "150iJZcmZlQW",
        "zljWXlXquHgp"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oZByfpftmcT"
      },
      "source": [
        "# Tutorial IV: Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N__G0gDAtmcU"
      },
      "source": [
        "<p>\n",
        "Bern Winter School on Machine Learning, 2022<br>\n",
        "Prepared by Mykhailo Vladymyrov.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knW3JBEDtmcU"
      },
      "source": [
        "In this session we will look at the convolutoin operation and try to build some intuition about it.\n",
        "Also we will look at one of the state-of-the art deep models, [Inception](https://arxiv.org/abs/1602.07261). It is designed to perform image recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7s9QvYetmcY"
      },
      "source": [
        "## 1. Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P-pLWxURfCY"
      },
      "source": [
        "colab = True # set to True is using google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G6x1ENecsyd"
      },
      "source": [
        "if colab:\n",
        "    %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjsvvAJatmcY"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "# We'll tell matplotlib to inline any drawn figures like so:\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGrGmp-5tmcV"
      },
      "source": [
        "### Download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL1BzlxC5PWy"
      },
      "source": [
        "if colab:\n",
        "    path = os.path.abspath('.')+'/material.tgz'\n",
        "    url = 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz'\n",
        "    p = tf.keras.utils.get_file(path, url)\n",
        "    assert p==path\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOpPZ2Hf5aB8"
      },
      "source": [
        "from utils import gr_disp\n",
        "from utils import inception"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVydrXfDRfCo"
      },
      "source": [
        "def show_graph(g=None, gd=None):\n",
        "    gr_disp.show_graph(g, gd)\n",
        "    %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Images"
      ],
      "metadata": {
        "id": "150iJZcmZlQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is an image?"
      ],
      "metadata": {
        "id": "krFtbykfXbTz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwVORI1Vtmca"
      },
      "source": [
        "## 3. Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0i6LmfYtmcb"
      },
      "source": [
        "In fully connected network all inputs of a layer are connected to all neurons of the following layer:\n",
        "<tr>\n",
        "    <td> <img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/Perceptron.png\" alt=\"drawing\" width=\"30%\"/></td> \n",
        "    <td> <img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/MLP.png\" alt=\"drawing\" width=\"50%\"/></td> \n",
        "</tr> \n",
        "<br>In convolutional nets the same holds for each neighbourhood, and the weights are shared:<br>\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/CNN1.png\" alt=\"drawing\" width=\"50%\"/><br>\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/CNN2.png\" alt=\"drawing\" width=\"50%\"/><br>\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/CNN3.png\" alt=\"drawing\" width=\"50%\"/><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_nODHgbtmcb"
      },
      "source": [
        "Let's see what a convolution is, and how it behaves."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Handkrafting filters"
      ],
      "metadata": {
        "id": "IKnDJ7lcNNsL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH2IPjiftmcc"
      },
      "source": [
        "#load image, convert to gray-scale and normalize\n",
        "img_raw = plt.imread('ML3/chelsea.jpg').mean(axis=2)[-256:, 100:356].astype(np.float32)\n",
        "img_raw = (img_raw-img_raw.mean())/img_raw.std()\n",
        "\n",
        "plt.imshow(img_raw, cmap='gray')\n",
        "plt.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi3rfvEitmce"
      },
      "source": [
        "x = tf.keras.layers.Input(dtype=tf.float32, shape=(None,None,1),name='img')\n",
        "flt = tf.keras.layers.Input(dtype=tf.float32, shape=(None,None,1,1), name='flt')[0]  # (h, w, n_in_ch, n_out_ch)\n",
        "\n",
        "stride = 1\n",
        "dilation = 1\n",
        "\n",
        "padding = 'VALID'\n",
        "y1 = tf.nn.conv2d(x , flt, strides=[1,stride,stride,1], dilations=[1,dilation,dilation,1], padding=padding, name='y1')\n",
        "y2 = tf.nn.conv2d(y1, flt, strides=[1,stride,stride,1], dilations=[1,dilation,dilation,1], padding=padding, name='y2')\n",
        "y3 = tf.nn.conv2d(y2, flt, strides=[1,stride,stride,1], dilations=[1,dilation,dilation,1], padding=padding, name='y3')\n",
        "y4 = tf.nn.conv2d(y3, flt, strides=[1,stride,stride,1], dilations=[1,dilation,dilation,1], padding=padding, name='y4')\n",
        "\n",
        "model = tf.keras.Model(inputs=[x, flt], outputs=[x*1, y1, y2, y3, y4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_convolved(img, flt):\n",
        "  img_raw4d = img[np.newaxis,...,np.newaxis]\n",
        "  \n",
        "  flt_mtx_np = np.array(flt_mtx, np.float32)\n",
        "  flt_mtx_np = flt_mtx_np[..., np.newaxis, np.newaxis]\n",
        "  \n",
        "  res = model((img_raw4d, flt_mtx_np))\n",
        "  res = [r[0,...,0].numpy() for r in res]\n",
        "\n",
        "  return res"
      ],
      "metadata": {
        "id": "K9dOPYUAJBJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's experiment with filter kernels:"
      ],
      "metadata": {
        "id": "8s8-jwn3YDpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flt_mtx = [\n",
        "    [ 0, 0, 0, 0, 0,],\n",
        "    [ 0, 0, 0, 0, 0,],\n",
        "    [ 0, 0, 1, 0, 0,],\n",
        "    [ 0, 0, 0, 0, 0,],\n",
        "    [ 0, 0, 0, 0, 0,],\n",
        "] # identity transformation\n",
        "\n",
        "ims_convolves = get_convolved(img_raw, flt_mtx)\n",
        "\n",
        "n = len(ims_convolves)\n",
        "fig, ax = plt.subplots(1, n+1, figsize=(n*4, 4))\n",
        "for col in range(n):\n",
        "    ax[col].imshow(ims_convolves[col], cmap='gray')  #, vmin=-3, vmax=3\n",
        "    ax[col].grid(False)\n",
        "    ax[col].set_title('conv %d'% col if col else 'raw')\n",
        "\n",
        "ax[n].imshow(flt_mtx, cmap='gray')\n",
        "ax[n].grid(False)\n",
        "_=ax[n].set_title('filter')"
      ],
      "metadata": {
        "id": "v17s7CUhMlQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Most common filters"
      ],
      "metadata": {
        "id": "57iJW9YGNk1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are most common filter kernels, in 1D:\n",
        "\n",
        "|filter type| effect|\n",
        "|-----|-----|\n",
        "|gaussian| bluring|\n",
        "|first derivative of gaussian|detection of edges|\n",
        "|second derivative of gaussian|detection of peaks|\n"
      ],
      "metadata": {
        "id": "JtrTJpS6NvnZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Van15HojduYJ"
      },
      "source": [
        "def gaussian(n=5):\n",
        "    x = np.linspace(-3, 3, n)\n",
        "    y = np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi)\n",
        "    return y\n",
        "\n",
        "def dgaussian(n=5):\n",
        "    x = np.linspace(-3, 3, n)\n",
        "    y = - 2 * x * np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi)\n",
        "    return y\n",
        "\n",
        "def ddgaussian(n=5):\n",
        "    x = np.linspace(-3, 3, n)\n",
        "    y = - 2 * (2*x**2 - 1) * np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi)\n",
        "    return y\n",
        "  \n",
        "def ddgaussian2d(n=5):\n",
        "    c = np.linspace(-3, 3, n)\n",
        "    r = np.asarray([[np.sqrt(xi**2+yi**2) for xi in c] for yi in c])\n",
        "    f = lambda x: (- 2 * (2*x**2 - 1) * np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi))\n",
        "\n",
        "    y = f(r)\n",
        "    y -= y.mean()\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OljkMi3jgkh1"
      },
      "source": [
        "n = 30\n",
        "\n",
        "gf = np.tile(gaussian(n)[np.newaxis], [n, 1])\n",
        "\n",
        "dgf = np.tile(dgaussian(n)[np.newaxis], [n, 1])\n",
        "\n",
        "ddgf = ddgaussian(n)\n",
        "ddgf -= ddgf.mean()\n",
        "ddgf = np.tile(ddgf[np.newaxis], [n, 1])\n",
        "\n",
        "ddgf2d = ddgaussian2d(n)\n",
        "rf2d = lambda:  np.random.normal(size=(5,5))\n",
        "\n",
        "\n",
        "plt.plot(gf[0], label=r'$g(x)$')\n",
        "plt.plot(dgf[0], label=r'$d g(x)/dx$')\n",
        "plt.plot(ddgf[0], label=r'$d^2 g(x)/dx^2$')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gabor filter is a gaussian in one direction, and a derivative of gaussin on the other:"
      ],
      "metadata": {
        "id": "sq_lrU4IP2ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gf.shape"
      ],
      "metadata": {
        "id": "A0x4ml1RFU6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wlTUAf60RXF"
      },
      "source": [
        "flt_mtx = gf*gf.transpose()\n",
        "\n",
        "#flt_mtx = rotate(flt_mtx, 30, reshape=False)\n",
        "\n",
        "plt.imshow(flt_mtx)\n",
        "plt.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#flt_mtx = gf*gf.transpose()    # gaussian filter\n",
        "#flt_mtx = dgf*gf.transpose()   # gabor filter, 1st derivative of gaussian (edge detection)\n",
        "#flt_mtx = ddgf*gf.transpose()  # 2nd derivative of gaussian (line detection)\n",
        "flt_mtx = ddgf2d               # 2nd derivative of gaussian, central symmetric, (spot detection)\n",
        "\n",
        "#flt_mtx = rotate(flt_mtx, 45, reshape=False)\n",
        "\n",
        "ims_convolves = get_convolved(img_raw, flt_mtx)\n",
        "\n",
        "n = len(ims_convolves)\n",
        "fig, ax = plt.subplots(1, n+1, figsize=(n*4, 4))\n",
        "for col in range(n):\n",
        "    ax[col].imshow(ims_convolves[col], cmap='gray')  #, vmin=-3, vmax=3\n",
        "    ax[col].grid(False)\n",
        "    ax[col].set_title('conv %d'% col if col else 'raw')\n",
        "\n",
        "ax[n].imshow(flt_mtx, cmap='gray')\n",
        "ax[n].grid(False)\n",
        "_=ax[n].set_title('filter')"
      ],
      "metadata": {
        "id": "ZiNvmAMkMuSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exercise 20 min"
      ],
      "metadata": {
        "id": "-QoRTghDYLDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with filters, try random, try to amplify some specific pattern eg whiskers or pupil, or perhaps make animation of filter effect depending on some parameter - e.g. size, angle, etc."
      ],
      "metadata": {
        "id": "YgcLuCrNYOLV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zljWXlXquHgp"
      },
      "source": [
        "## 4. Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZBfo0m40vB6"
      },
      "source": [
        "In last session we used fully connected network to clasify digits.\n",
        "Try to build the convolutional network: use three convolutional layers, then flatten the ouput and apply 1 fully connected.\n",
        "You can use the following helper function. Notice: there is a stride parameter. It allows to effectively downscale the feature maps.\n",
        "To get an understanding of different convolution types, check the <a href=\"https://github.com/vdumoulin/conv_arithmetic\">animations here</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM-OC3L4wyTn"
      },
      "source": [
        "You can start with something like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3n54VPw-C5"
      },
      "source": [
        "...\n",
        "x_train = x_train_2d[..., np.newaxis]  # we need additional channel dimension, to get 4D (BHWC) dataset\n",
        "\n",
        "....\n",
        "\n",
        "l1 = tf.keras.layers.Conv2D(16, 3, name = 'C1', kernel_initializer='he_uniform')(x)\n",
        "l2 = tf.keras.layers.Conv2D(32, 3, strides=2, name = 'C2', kernel_initializer='he_uniform')(l1)\n",
        "l3 = tf.keras.layers.Conv2D(32, 3, strides=2, name = 'C3', kernel_initializer='he_uniform')(l2)\n",
        "\n",
        "l3_f = tf.keras.layers.Flatten()(l3)\n",
        "\n",
        "l4 = tf.keras.layers.Dense(units=32, name='l4', activation='tf.nn.relu')(l3_f)\n",
        "l5 = tf.keras.layers.Dense(units=10, name='l5', activation='softmax')(l4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEnrC5c0z-Ci"
      },
      "source": [
        "Play with layer parameters. Can you get better performance than in fully connected network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPrth-Ertmck"
      },
      "source": [
        "## 5. Load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check some pretrained models"
      ],
      "metadata": {
        "id": "5yzrNhX2ZPGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.InceptionV3(include_top=True) # ResNet50V2"
      ],
      "metadata": {
        "id": "Y6G25jyJiIVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBTGwxsqRfDJ"
      },
      "source": [
        "tf.keras.utils.plot_model(base_model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPzKLLPitmcr"
      },
      "source": [
        "## 6. Test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJIfPj7Wtmcs"
      },
      "source": [
        "We will use one image to check model. `img_preproc` is croped to 299x299 pixels and slightly transformed to be used as imput for the model using `inception.prepare_training_img`. `inception.training_img_to_display` is then used to convert it to displayable one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9HRNGmstmcs"
      },
      "source": [
        "sz = 299\n",
        "img_raw = plt.imread('ML3/chelsea.jpg')\n",
        "\n",
        "img_crop = img_raw.copy()[:sz, 100:100+sz]\n",
        "\n",
        "_, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "axs[0].imshow(img_raw)\n",
        "axs[0].grid(False)\n",
        "axs[1].imshow(img_crop)\n",
        "axs[1].grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to apply same scaling to the input asa was done for training samples. This is done with a `preprocess_input` method corresponding to a model"
      ],
      "metadata": {
        "id": "DLpxv4yKS-3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_preproc = tf.keras.applications.inception_v3.preprocess_input(img_crop)   #resnet_v2"
      ],
      "metadata": {
        "id": "ShlKcFtRS9-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7fpmx0btmcu"
      },
      "source": [
        "We then obtain probabilities of each class on this image:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = base_model.predict(img_preproc[np.newaxis])"
      ],
      "metadata": {
        "id": "XvPS6LBui7SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.applications.inception_v3.decode_predictions(pred, top=5)  #resnet_v2"
      ],
      "metadata": {
        "id": "W-Q6tcywi5n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = tf.keras.applications.inception_v3.decode_predictions(pred, top=-1)  #resnet_v2\n",
        "plt.semilogy([prob for class_id, class_name, prob in all_preds[0]], '.');"
      ],
      "metadata": {
        "id": "tL3fFVrrTmeM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}