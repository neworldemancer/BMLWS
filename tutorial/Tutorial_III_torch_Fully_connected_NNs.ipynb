{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial III: Fully connected NNs\n",
        "\n",
        "<p>\n",
        "Bern Winter School on Machine Learning, 2024<br>\n",
        "Prepared by Mykhailo Vladymyrov and Matthew Vowels.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
        "\n",
        "In this session we will create a fully-connected neural network to perform handwritten digit recognition using pytorch"
      ],
      "metadata": {
        "id": "NTK34uxMjMYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load necessary libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "kmjKrW6WjS3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import requests\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "# URL of the file to download\n",
        "url = 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz'\n",
        "\n",
        "# Path where the file will be saved\n",
        "path = os.path.abspath('.') + '/material.tgz'\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "assert response.status_code == 200, \"Download failed\"\n",
        "with open(path, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Check if the path is correct\n",
        "assert os.path.exists(path), \"File not found\"\n",
        "\n",
        "# Extract the tar file\n",
        "tar = tarfile.open(path, \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "f8lKArr6jSgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training loop"
      ],
      "metadata": {
        "id": "duYLZIq-vF28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training as we just saw is done iteratively, by adjusting the model parameters.\n",
        "\n",
        "We perform optimization several times for all traininng dataset. Going through all this dataset is refered to as 'epoch'.\n",
        "\n",
        "When we do training its usually done in two loops. In outer loop we iterate over all epochs. For each epoch we usually split the dataset into small chuncks, batches, and optimization it performed for all of those.\n",
        "\n",
        "It is important that data doesn't go to the training pipeline in same order. So the overall scheme looks like this (pseudocode):\n",
        "\n",
        "\n",
        "```\n",
        "x,y = get_training_data()\n",
        "for epoch in range(number_epochs):\n",
        "   x_shfl,y_shfl = shuffle(x,y)\n",
        "   \n",
        "   for mb_idx in range(number_minibatches_in_batch):\n",
        "       x_mb,y_mb = get_minibatch(x_shfl,y_shfl, mb_idx)\n",
        "       \n",
        "       optimize_on(data=x_mb, labels=y_mb)\n",
        "```"
      ],
      "metadata": {
        "id": "hZvqLoJF5jy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Bulding blocks of a neural network"
      ],
      "metadata": {
        "id": "lmsRNxYLvBvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural network consists of layers of neurons. Each neuron performs 2 operations.\n",
        "\n",
        "1. Calculate the linear transformation of the input vector $\\mathbf{x}_i$:\n",
        "$$z_i = \\mathbf{w} \\cdot \\mathbf{x}_i + b = \\sum_j {w_j x_{ij}} + b$$ where $\\mathbf{w}$ is a weight vector and $b$ - a bias, and $j$ traverses each dimension of the input vector\n",
        "2. Perform the nonlinear transformation of the result using the activation function $f$ $$y_i = f(z_i)$$\n",
        "\n",
        "In a fully connected neural network, each layer is a set of N neurons, performing different transformations of the input $\\mathbf{x}_i$ of the same layer, now producing an output **vector** $ \\mathbf{y} _i = f(\\mathbf{z}_i) = f(W\\mathbf{x}_i + \\mathbf{b})$ now with a bias vector $\\mathbf{b}$ and a * *matrix** of weights $W$.\n",
        "\n",
        "Since the output of each layer constitutes the input to the next layer, we can write for layer $l$: $$\\mathbf{x}^l_i = f^{l-1}(\\mathbf{W}^{ l-1} \\mathbf{x}^{ l-1}_i + \\mathbf{b}^{l-1})$$ where $\\mathbf{x}_i^{l=0}$ is the vector d 'network input for data point $i$."
      ],
      "metadata": {
        "id": "x_-ZD3CC8A4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/MLP.png\" alt=\"drawing\" width=\"50%\"/>\n",
        "\n",
        "To simplify building the network, we'll define a helper function, creating neuron layer with given number of outputs:"
      ],
      "metadata": {
        "id": "uZx9PQEj8GTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, units, activation=None):\n",
        "        \"\"\"\n",
        "        Fully connected layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        units : int\n",
        "            Number of output neurons\n",
        "        activation : None or callable, optional\n",
        "            Non-linear activation function (e.g., torch.nn.functional.relu)\n",
        "        \"\"\"\n",
        "        super(Dense, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize the weights and biases\n",
        "        self.linear = nn.Linear(in_features=units, out_features=units)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Apply linear transformation\n",
        "        outputs = self.linear(inputs)\n",
        "\n",
        "        # Apply activation function if provided\n",
        "        if self.activation is not None:\n",
        "            outputs = self.activation(outputs)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "DK65Jnay77j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of multi-class classification, in the last layer, we can use the *softmax* transformation as a non-linear transformation. The softmax for the $j$th element of $\\mathbf{z}_i$ is $$y_{ij} = \\frac{e^{z_{ij}}}{\\sum_{k=1}^{K} e^{z_{ik}}}$$ where $K$ is the total number of classes.\n",
        "\n",
        "\n",
        "For example. before softmax $$[1.0, 2.0, 3.0, \\mathbf{4.0} ]$$ and after: $$[0.0321, 0.0871, 0.2369, \\mathbf{0.6439}]$$ (now the sum is equal to one)\n",
        "\n",
        "We can also now compare the fundamental truth which could be $$[0,0,0,\\mathbf{1}]$$ That is to say that class 4 is the good class and that the network predicted correctly.\n",
        "\n",
        "\n",
        "\n",
        "The **optimization/loss function** in multiclass classification problems is multiclass cross-entropy:\n",
        "$$\\mathcal{L}_i = - \\sum_{k=1}^K y^*_{ik} \\log(y_{ik})$$ where $y^*_{ik}$ is the $k $th component of the unique true label vector $\\mathbf{y}^*_i$, and $y_{ik}$ is the $k$th component of the predicted probability vector $\\mathbf{y}_i$.\n",
        "\n",
        "$$[0.0321, 0.0871, 0.2369, \\mathbf{0.6439}] = [y_{i1}, y_{i2}, y_{i3}, y_{i4}]$$"
      ],
      "metadata": {
        "id": "GQzdb_ZP8fxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Structure of a neural network"
      ],
      "metadata": {
        "id": "U0I6wPiH8lIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, n_input, n_hiddens, n_output):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.ls = []\n",
        "        n_prev = n_input\n",
        "        for i, n_out in enumerate(n_hiddens):\n",
        "          l = nn.Linear(n_prev, n_out)\n",
        "          n_prev = n_out\n",
        "          self.add_module(f'lin_{i}_{n_out}', l)\n",
        "          self.ls.append(l)\n",
        "\n",
        "        self.lout = nn.Linear(n_prev, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for li in self.ls:\n",
        "          h = li(h)\n",
        "          h = torch.relu(h)\n",
        "\n",
        "        logits = self.lout(h)\n",
        "        # Apply softmax activation\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Prediction: argmax for classification\n",
        "        pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "        return logits, probs, pred\n"
      ],
      "metadata": {
        "id": "Qo8IDRWI8ohv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the data\n",
        "\n",
        "First we will load the data: 60000 training images and 10000 images for validation with some included data transformations.\n",
        "\n",
        "Each image is a 28x28 pixels. For this model we will interpret it as a 1D array of 784 elements."
      ],
      "metadata": {
        "id": "OyxjipoN9dfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                                transforms.Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print('Train dataset shape:', len(trainset), 'total images and labels')\n",
        "print('Test dataset shape:', len(testset), 'total images and labels')"
      ],
      "metadata": {
        "id": "O-EivCFK87eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for el in trainset:\n",
        "  x, y = el\n",
        "  print(x.shape, y)\n",
        "  break"
      ],
      "metadata": {
        "id": "bABHqvh_xY60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for el in trainloader:\n",
        "  x, y = el\n",
        "  print(x.shape, y.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "prPyGDvxxw-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's take a look at some of the example images"
      ],
      "metadata": {
        "id": "Z8Rjh7My9pFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, labels, num_images=5):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        # Reshape the flattened images back to 28x28 for display\n",
        "        plt.imshow(images[i].numpy().reshape(28, 28), cmap='gray')\n",
        "        plt.xlabel('Label: {}'.format(labels[i].item()))\n",
        "    plt.show()\n",
        "\n",
        "# Function to get random images and labels from a dataset\n",
        "def get_random_images(loader, num_images=5):\n",
        "    dataiter = iter(loader)\n",
        "    images, labels = next(dataiter)\n",
        "    indices = list(range(len(images)))\n",
        "    random_indices = random.sample(indices, num_images)\n",
        "    random_images = images[random_indices]\n",
        "    random_labels = labels[random_indices]\n",
        "    return random_images, random_labels\n",
        "\n",
        "# Get random images and labels from the training set\n",
        "random_images, random_labels = get_random_images(trainloader, num_images=5)\n",
        "# Show images\n",
        "show_images(random_images, random_labels, num_images=5)\n",
        "\n",
        "# and for the test set:\n",
        "random_images, random_labels = get_random_images(testloader, num_images=5)\n",
        "show_images(random_images, random_labels, num_images=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "wIgya80r9ABY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the dimensions of the data"
      ],
      "metadata": {
        "id": "ZM0bbfI4-4Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Inspect the dimensions of the sample elements\n",
        "print(\"Shape of the images:\", random_images.shape)\n",
        "print(\"Shape of the labels:\", random_labels.shape)\n",
        "print(random_labels)"
      ],
      "metadata": {
        "id": "1L2cggGn948g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Bulding a neural network\n",
        "\n",
        "Number of inputs for neurons will be given by input data, i.e. image dims (flattened), size. Output - by number of classes, 10 in our case."
      ],
      "metadata": {
        "id": "z6jMPCtL_3TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = []  # [10]\n",
        "model = MyModel(n_input=784, n_hiddens=n_hidden, n_output=10)  # 784 input features for 28x28 images, 10 output classes\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (e.g., Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "EYGX-9RP-6B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_npars(model)"
      ],
      "metadata": {
        "id": "jmzF7K2VJetY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a training function:"
      ],
      "metadata": {
        "id": "f5o71gymAA9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()  # reset gradients\n",
        "        output, _, _ = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()    # compute gradients\n",
        "        optimizer.step()   # update parameters with gradients\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(trainloader)"
      ],
      "metadata": {
        "id": "dTrUXsn1_04A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a testing function:"
      ],
      "metadata": {
        "id": "bdMh6wDbADtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, testloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    n_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            output, prob, pred = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            running_loss += loss.item()\n",
        "            correct = torch.sum(pred == labels)\n",
        "            n_correct =+ correct\n",
        "    return running_loss / len(testloader)  #, n_correct/len(testloader)/testloader.batch_size"
      ],
      "metadata": {
        "id": "u7h2JtuFADQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testloader.batch_size"
      ],
      "metadata": {
        "id": "ZFsha_J42uPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "OGvXGVvaAH_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, criterion, optimizer)\n",
        "    test_loss = test(model, testloader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "id": "B34VjSa2AGsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the loss curves"
      ],
      "metadata": {
        "id": "XjNpSOZ8BdTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(range(num_epochs), train_losses, label='Training Loss')\n",
        "ax.plot(range(num_epochs), test_losses, label='Test Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tQNq_xOGAJc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.parameters():\n",
        "  print(np.prod(list(p.shape)))"
      ],
      "metadata": {
        "id": "_z0KQzL9_lOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_npars(model):\n",
        "  return np.sum([np.prod(list(p.shape)) for p in model.parameters()])"
      ],
      "metadata": {
        "id": "VTcudsB5Hkoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_npars(model)"
      ],
      "metadata": {
        "id": "zPUM_HJ6Hw1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate network performance using a basic accuracy metric:\n",
        "\n",
        "$$\\mbox{Acc}= \\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}(y_i = y^*_i)$$\n",
        "\n",
        "where $\\mathbb{I}()$ is the indicator function.\n",
        "\n",
        "\n",
        "Also investigate what an incorrect prediction looks like..."
      ],
      "metadata": {
        "id": "WJxx2VgNBrJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    images_list = []\n",
        "    with torch.no_grad():\n",
        "        for images, label in loader:\n",
        "            output, _, _ = model(images)\n",
        "            pred = output.argmax(dim=1)\n",
        "            predictions.extend(pred.tolist())\n",
        "            labels.extend(label.tolist())\n",
        "            images_list.extend(images)\n",
        "    return predictions, labels, images_list\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = sum(pred == label for pred, label in zip(predictions, labels))\n",
        "    total = len(labels)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Get predictions, labels, and images\n",
        "predictions, labels, images_list = get_predictions(model, testloader)\n",
        "correct = [pred == label for pred, label in zip(predictions, labels)]\n",
        "\n",
        "accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "ogO__Cv9BtYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate a correct and incorrect example and visualize:"
      ],
      "metadata": {
        "id": "E43y5wzgEkO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Find the index of the first incorrect and correct prediction\n",
        "wrong_idx = next((i for i, correct in enumerate(correct) if not correct), None)\n",
        "correct_idx = next((i for i, correct in enumerate(correct) if correct), None)\n",
        "\n",
        "# Function to display an image\n",
        "def display_image(image, title):\n",
        "    plt.imshow(image.numpy().reshape(28, 28), cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Display the first incorrect digit (if any incorrect are found)\n",
        "if wrong_idx is not None:\n",
        "    print(f\"First incorrect digit is at index: {wrong_idx}\")\n",
        "    display_image(images_list[wrong_idx], f\"Predicted: {predictions[wrong_idx]}, Actual: {labels[wrong_idx]}\")\n",
        "else:\n",
        "    print(\"No incorrect predictions found\")\n",
        "\n",
        "# Display the first correct digit (if any correct are found)\n",
        "if correct_idx is not None:\n",
        "    print(f\"First correct digit is at index: {correct_idx}\")\n",
        "    display_image(images_list[correct_idx], f\"Predicted: {predictions[correct_idx]}, Correct: {labels[correct_idx]}\")\n",
        "else:\n",
        "    print(\"No correct predictions found\")"
      ],
      "metadata": {
        "id": "M_XRml4MB4AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Visualizing the Parameters\n",
        "\n",
        "The learned model parameters W1 are a matrix of weights that show importance of each input pixel (784) for each of the 10 outputs."
      ],
      "metadata": {
        "id": "IvrCUICwExDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1, b1 = model.l1.weight.data, model.l1.bias.data\n",
        "print(w1.shape, b1.shape)\n",
        "\n",
        "w1_reshaped = w1.view(n_hidden, 28, 28)  # Adjust the view dimensions as per your layer's configuration\n",
        "\n",
        "# Plotting the weights\n",
        "_, axs = plt.subplots(1, n_hidden, figsize=(13, 5))\n",
        "for i in range(10):\n",
        "    axs[i].imshow(w1_reshaped[i].numpy(), cmap='plasma', interpolation='nearest')\n",
        "    axs[i].grid(False)\n",
        "    axs[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kpuGXde1DL1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we classify images into 10 classes. But think of it: does the network know, or need to know that those were images? For the network each image is just a 784 values. And it finds that there is a patten.\n",
        "\n",
        "Same way one can feed any other bunch of numbers, and the network will try it's best to fugure out a relation pattern between those.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zH6SLtqXLSwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    images_list = []\n",
        "    with torch.no_grad():\n",
        "        for images, label in loader:\n",
        "            output, _ = model(images)\n",
        "            pred = output.argmax(dim=1)\n",
        "            predictions.extend(pred.tolist())\n",
        "            labels.extend(label.tolist())\n",
        "            images_list.extend(images)\n",
        "    return predictions, labels, images_list\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct = sum(pred == label for pred, label in zip(predictions, labels))\n",
        "    total = len(labels)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "KgZYX8zQh-gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, labels, images_list = get_predictions(model, testloader)  # TEST\n",
        "\n",
        "accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "Kk6Y9MM1h-l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Exercise 1\n",
        "\n",
        "Build a network with two layers, first with `tf.nn.relu` ReLU activation and 1500 neurons and second one with 10 and softmax activation. Start with `learning_rate` of 0.001 and find optimal value."
      ],
      "metadata": {
        "id": "Hjw4ZIz1LZZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Gradients visualization"
      ],
      "metadata": {
        "id": "gDS2USefLdp_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PBunQ5VrE-9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will display several images, and corresponding gradients of maximal output activation, as well as all activations. This might help better understand how our network processes the imput data."
      ],
      "metadata": {
        "id": "el8EtuqKMC_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "grads_all_classes = []\n",
        "\n",
        "\n",
        "# The code uses torch.autograd.grad to compute the gradients of the model's predictions with respect to the input image.\n",
        "# This essentially answers the question, \"How does changing each pixel in the input image affect the model's confidence in its prediction for each class (digit)?\"\n",
        "\n",
        "# For each digit class (0 to 9), the code computes a separate gradient map.\n",
        "# It does so by setting grad_outputs to a tensor that is all zeros except for a one at the current class's position.\n",
        "# This way, the computed gradient reflects how much each pixel in the input image contributes to the model's prediction for that specific class.\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Choose an index and select a single image and label\n",
        "idx = 1\n",
        "inp_v = images[idx:idx+1]  # Selecting the image\n",
        "img = inp_v.squeeze()  # The original 2D image for display\n",
        "\n",
        "# Enable gradients for input\n",
        "inp_v.requires_grad = True\n",
        "\n",
        "# Forward pass and compute gradients\n",
        "model.eval()\n",
        "with torch.set_grad_enabled(True):\n",
        "    preds = model(inp_v)[0]  # Assuming the model returns a tuple (output, argmax)\n",
        "    for i in range(preds.size(1)):  # Iterate over each class\n",
        "        grad_outputs = torch.zeros_like(preds)\n",
        "        grad_outputs[0, i] = 1\n",
        "        grads = torch.autograd.grad(outputs=preds, inputs=inp_v, grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "        grads_all_classes.append(grads)\n",
        "\n",
        "# Stack all gradients and reshape to desired format\n",
        "grads_stacked = torch.stack(grads_all_classes).view(preds.size(1), 28, 28).detach().numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "5WGirHW5K-On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the following subplots shows the gradient map for a particular class. These maps can be interpreted as heatmaps indicating which regions of the input image are most critical for the model's prediction of that particular digit.\n",
        "Brighter areas in a gradient map indicate pixels that, if changed, would have a larger impact on the model's prediction for that class. This can give insights into what features the model is focusing on for each class.\n",
        "\n",
        "\n",
        "https://medium.com/geekculture/how-visualizations-help-make-sense-of-deep-learning-a408ab00688f\n"
      ],
      "metadata": {
        "id": "bZ2y6f3BaWBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each of the following subplots shows the gradient map for a particular class.\n",
        "# These maps can be interpreted as heatmaps indicating which regions of the input image are most critical for the model's prediction of that particular digit.\n",
        "# Brighter areas in a gradient map indicate pixels that, if changed, would have a larger impact on the model's prediction for that class.\n",
        "# This can give insights into what features the model is focusing on for each class.\n",
        "\n",
        "# Reshape the flattened image back to 2D for display\n",
        "img_2d = img.view(28, 28).numpy()\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(1, 11, figsize=(4.1 * 11, 4))\n",
        "axs[0].imshow(img_2d, cmap='gray')  # Display the reshaped original image\n",
        "axs[0].set_title('raw')\n",
        "vmin, vmax = grads_stacked.min(), grads_stacked.max()\n",
        "\n",
        "\n",
        "for i, g in enumerate(grads_stacked):\n",
        "    axs[i + 1].imshow(g, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "    axs[i + 1].set_title(r'$\\frac{\\partial\\;P(digit\\,%d)}{\\partial\\;input}$' % i, fontdict={'size': 16})\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_UQWUFI4Magu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Exercise 2\n",
        "### Group 1\n",
        "Build the network with 3 or more layers. Try to get test accuracy >98.5%.\n",
        "Better to copy and modify the previous code so that you can compare against the code/results above.\n",
        "\n",
        "\n",
        "### Group 2\n",
        "\n",
        "You can repeat the same for a regression problem: here you will have just 1 output in the last layer, with no activation - to predict continuous unboud range. You can use the `mse` or `mae` loss. Compare results with a baseline linear / random forest model.\n",
        "\n",
        "How many parameters does your model have as compared to number of samples?"
      ],
      "metadata": {
        "id": "hUXFJ9H8bUJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# example solution code for Group 2:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Boston Housing dataset\n",
        "cali = fetch_california_housing()\n",
        "X, y = cali.data, cali.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Define a batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Create DataLoaders for training and testing data\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "class MyRegressor(nn.Module):\n",
        "    def __init__(self, n_input, n_hidden, n_output):\n",
        "        super(MyRegressor, self).__init__()\n",
        "        # Layer 1: 10 inputs -> 4, with sigmoid activation\n",
        "        self.l1 = nn.Linear(n_input, n_hidden)\n",
        "\n",
        "        # Layer 2: 4 inputs -> n_output, with softmax activation\n",
        "        self.l2 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first layer and sigmoid activation\n",
        "        x = torch.sigmoid(self.l1(x))\n",
        "        return self.l2(x), self.l2(x)  # ugly workaround to make it compatible with earlier functions\n",
        "\n",
        "\n",
        "\n",
        "n_input = x_train.shape[1]  # Number of features in the Boston Housing dataset\n",
        "n_hidden = 4  # Example number of hidden units\n",
        "n_output = 1  # Regression output\n",
        "\n",
        "model = MyRegressor(n_input=n_input, n_hidden=n_hidden, n_output=n_output)\n",
        "\n",
        "criterion = nn.MSELoss()  # For regression, Mean Squared Error Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, criterion, optimizer)\n",
        "    test_loss = test(model, testloader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(range(num_epochs), train_losses, label='Training Loss')\n",
        "ax.plot(range(num_epochs), test_losses, label='Test Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qFH8uQbtMx6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Train the random forest model\n",
        "rf.fit(x_train, y_train[:,0])\n",
        "\n",
        "# Predict on the test set\n",
        "rf_predictions = rf.predict(x_test)\n",
        "\n",
        "# Calculate the test loss (MSE)\n",
        "rf_test_loss = mean_squared_error(y_test, rf_predictions)\n",
        "print(f'Random Forest Test Loss: {rf_test_loss:.4f}')"
      ],
      "metadata": {
        "id": "JWQj9BKmb6cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-f8DIi_VcQhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Opvg5YH_eIHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}