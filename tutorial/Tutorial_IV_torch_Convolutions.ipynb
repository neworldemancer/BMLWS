{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "Tutorial_IV_tf2_Convolutions.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "l7s9QvYetmcY",
    "150iJZcmZlQW",
    "zljWXlXquHgp"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oZByfpftmcT"
   },
   "source": [
    "# Tutorial IV: Convolutions in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N__G0gDAtmcU"
   },
   "source": [
    "<p>\n",
    "Bern Winter School on Machine Learning, 2024<br>\n",
    "Prepared by Mykhailo Vladymyrov and Matthew Vowels.\n",
    "</p>\n",
    "\n",
    "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knW3JBEDtmcU"
   },
   "source": [
    "In this session we will look at the convolution operation and try to build some intuition about it.\n",
    "Also we will look at one of the state-of-the art deep models, [Inception](https://arxiv.org/abs/1602.07261). It is designed to perform image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7s9QvYetmcY"
   },
   "source": [
    "## 1. Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5P-pLWxURfCY"
   },
   "source": [
    "colab = True # set to True is using google colab"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cjsvvAJatmcY"
   },
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" \n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.utils import download_from_url\n",
    "\n",
    "    \n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# We'll tell matplotlib to inline any drawn figures like so:\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.abspath('.')+'material.tgz')\n",
    "url = 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz'\n",
    "# p = tf.keras.utils.get_file(path, url)\n",
    "# Download compressed file with torch utils\n",
    "\n",
    "p = download_from_url(url=url, path=path)\n",
    "\n",
    "assert p==path\n",
    "tar = tarfile.open(path, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Images"
   ],
   "metadata": {
    "id": "150iJZcmZlQW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is an image?"
   ],
   "metadata": {
    "id": "krFtbykfXbTz"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwVORI1Vtmca"
   },
   "source": [
    "## 3. Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0i6LmfYtmcb"
   },
   "source": [
    "In fully connected network all inputs of a layer are connected to all neurons of the following layer:\n",
    "<tr>\n",
    "    <td> <img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/Perceptron.png\" alt=\"drawing\" width=\"30%\"/></td> \n",
    "    <td> <img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/MLP.png\" alt=\"drawing\" width=\"50%\"/></td> \n",
    "</tr> \n",
    "<br>In convolutional nets the same holds for each neighbourhood, and the weights are shared:<br>\n",
    "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/CNN1.png\" alt=\"drawing\" width=\"50%\"/><br>\n",
    "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/CNN2.png\" alt=\"drawing\" width=\"50%\"/><br>\n",
    "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/CNN3.png\" alt=\"drawing\" width=\"50%\"/><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_nODHgbtmcb"
   },
   "source": [
    "Let's see what a convolution is, and how it behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1. Handkrafting filters"
   ],
   "metadata": {
    "id": "IKnDJ7lcNNsL"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dH2IPjiftmcc"
   },
   "source": [
    "#load image, convert to gray-scale and normalize\n",
    "img_raw = plt.imread('ML3/chelsea.jpg')  # load RGB image (HWC)\n",
    "img_raw = img_raw.mean(axis=2)  # convert to gray-scale by averaging over color channels\n",
    "img_raw = img_raw[-256:, 100:356]  # crop to 256x256 pixels\n",
    "img_raw = img_raw.astype(np.float32)  # convert to float32\n",
    "img_raw = (img_raw-img_raw.mean())/img_raw.std()  # normalize to zero mean and unit variance\n",
    "\n",
    "plt.imshow(img_raw, cmap='gray')  # show image\n",
    "plt.grid(False)  # disable grid"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def conv_2d(x, flt):\n",
    "    # x: B, C, H, W\n",
    "    # flt: Co, Ci, Hf, Wf\n",
    "    return F.conv2d(input=x, weight=flt, stride=1, padding=0, dilation=1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x, flt):\n",
    "        print(x.shape, flt.shape)\n",
    "        y1 = conv_2d(x, flt)\n",
    "        y2 = conv_2d(y1, flt)\n",
    "        y3 = conv_2d(y2, flt)\n",
    "        y4 = conv_2d(y3, flt)\n",
    "        return x, y1, y2, y3, y4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Model().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_convolved(img, flt):\n",
    "    img_raw4d = img[np.newaxis,np.newaxis] # add batch (1) and channel (1) dimensions\n",
    "    \n",
    "    flt_mtx_np = np.array(flt, np.float32)\n",
    "    flt_mtx_np = flt_mtx_np[np.newaxis, np.newaxis] # add N channels out (1) and N channels in (1) dimensions\n",
    "    \n",
    "    img_raw4d_t = torch.from_numpy(img_raw4d).to(device)\n",
    "    flt_mtx_t = torch.from_numpy(flt_mtx_np).to(device) \n",
    "    \n",
    "    res = model(img_raw4d_t, flt_mtx_t)\n",
    "    res = [r.detach().cpu().numpy() for r in res]  # disable gradient tracking, move to cpu, convert to numpy\n",
    "    res = [r[0,0] for r in res]  # remove batch and channel dimensions, only one channel in the output\n",
    "    \n",
    "    return res"
   ],
   "metadata": {
    "id": "K9dOPYUAJBJj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's experiment with filter kernels:"
   ],
   "metadata": {
    "id": "8s8-jwn3YDpd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "flt_mtx = [\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "    [ 0, 0, 1, 0, 0,],\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "    [ 0, 0, 0, 0, 0,],\n",
    "] # identity transformation\n",
    "\n",
    "ims_convolved = get_convolved(img_raw, flt_mtx)\n",
    "\n",
    "n = len(ims_convolved)\n",
    "fig, ax = plt.subplots(1, n+1, figsize=(n*4, 4))\n",
    "for col in range(n):\n",
    "    ax[col].imshow(ims_convolved[col], cmap='gray')  #, vmin=-3, vmax=3\n",
    "    ax[col].grid(False)\n",
    "    ax[col].set_title('conv %d'% col if col else 'raw')\n",
    "\n",
    "ax[n].imshow(flt_mtx, cmap='gray')\n",
    "ax[n].grid(False)\n",
    "_=ax[n].set_title('filter')"
   ],
   "metadata": {
    "id": "v17s7CUhMlQz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. experiment with different filters and understand what they do, e.g.:<br>\n",
    "- identity transformation\n",
    "- identity transformation with positive non-unit values\n",
    "- identity transformation with negative unit value\n",
    "- identity transformation off center\n",
    "- blurring with box filter\n",
    "- edge detection with + and - bands\n",
    "- try whatever you like\n",
    "\n",
    "2. experiment with convolution parameters: <br>\n",
    "- padding = 1, 2, 3\n",
    "- stride = 2\n",
    "- dilation = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2. Most common filters"
   ],
   "metadata": {
    "id": "57iJW9YGNk1m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are most common filter kernels, in 1D:\n",
    "\n",
    "|filter type| effect|\n",
    "|-----|-----|\n",
    "|gaussian| bluring|\n",
    "|first derivative of gaussian|detection of edges|\n",
    "|second derivative of gaussian|detection of peaks|\n"
   ],
   "metadata": {
    "id": "JtrTJpS6NvnZ"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Van15HojduYJ"
   },
   "source": [
    "def gaussian(n=5):\n",
    "    x = np.linspace(-3, 3, n)\n",
    "    y = np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi)\n",
    "    return y\n",
    "\n",
    "def dgaussian(n=5):\n",
    "    x = np.linspace(-3, 3, n)\n",
    "    y = - 2 * x * np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi)\n",
    "    return y\n",
    "\n",
    "def ddgaussian(n=5):\n",
    "    x = np.linspace(-3, 3, n)\n",
    "    y = - 2 * (2*x**2 - 1) * np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi)\n",
    "    return y\n",
    "  \n",
    "def ddgaussian2d(n=5):\n",
    "    c = np.linspace(-3, 3, n)\n",
    "    r = np.asarray([[np.sqrt(xi**2+yi**2) for xi in c] for yi in c])\n",
    "    f = lambda x: (- 2 * (2*x**2 - 1) * np.exp(-x**2 * 0.5) / np.sqrt(2*np.pi))\n",
    "\n",
    "    y = f(r)\n",
    "    y -= y.mean()\n",
    "    return y"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OljkMi3jgkh1"
   },
   "source": [
    "n = 30\n",
    "\n",
    "gf = np.tile(gaussian(n)[np.newaxis], [n, 1])\n",
    "\n",
    "dgf = np.tile(dgaussian(n)[np.newaxis], [n, 1])\n",
    "\n",
    "ddgf = ddgaussian(n)\n",
    "ddgf -= ddgf.mean()\n",
    "ddgf = np.tile(ddgf[np.newaxis], [n, 1])\n",
    "\n",
    "ddgf2d = ddgaussian2d(n)\n",
    "rf2d = lambda:  np.random.normal(size=(5,5))\n",
    "\n",
    "\n",
    "plt.plot(gf[0], label=r'$g(x)$')\n",
    "plt.plot(dgf[0], label=r'$d g(x)/dx$')\n",
    "plt.plot(ddgf[0], label=r'$d^2 g(x)/dx^2$')\n",
    "plt.legend();"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gabor filter is a gaussian in one direction, and a derivative of gaussin on the other:"
   ],
   "metadata": {
    "id": "sq_lrU4IP2ot"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gf.shape"
   ],
   "metadata": {
    "id": "A0x4ml1RFU6l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-wlTUAf60RXF"
   },
   "source": [
    "flt_mtx = gf*gf.transpose()\n",
    "\n",
    "#flt_mtx = rotate(flt_mtx, 30, reshape=False)\n",
    "\n",
    "plt.imshow(flt_mtx)\n",
    "plt.grid(False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flt_mtx = gf*gf.transpose()    # gaussian filter\n",
    "#flt_mtx = dgf*gf.transpose()   # gabor filter, 1st derivative of gaussian (edge detection)\n",
    "#flt_mtx = ddgf*gf.transpose()  # 2nd derivative of gaussian (line detection)\n",
    "#flt_mtx = ddgf2d               # 2nd derivative of gaussian, central symmetric, (spot detection)\n",
    "\n",
    "#flt_mtx = rotate(flt_mtx, 45, reshape=False)\n",
    "\n",
    "ims_convolved = get_convolved(img_raw, flt_mtx)\n",
    "\n",
    "n = len(ims_convolved)\n",
    "fig, ax = plt.subplots(1, n+1, figsize=(n*4, 4))\n",
    "for col in range(n):\n",
    "    ax[col].imshow(ims_convolved[col], cmap='gray')  #, vmin=-3, vmax=3\n",
    "    ax[col].grid(False)\n",
    "    ax[col].set_title('conv %d'% col if col else 'raw')\n",
    "\n",
    "ax[n].imshow(flt_mtx, cmap='gray')\n",
    "ax[n].grid(False)\n",
    "_=ax[n].set_title('filter')"
   ],
   "metadata": {
    "id": "ZiNvmAMkMuSr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Exercise 20 min"
   ],
   "metadata": {
    "id": "-QoRTghDYLDX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experiment with filters, try random, try to amplify some specific pattern eg whiskers or pupil, or perhaps make animation of filter effect depending on some parameter - e.g. size, angle, etc."
   ],
   "metadata": {
    "id": "YgcLuCrNYOLV"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zljWXlXquHgp"
   },
   "source": [
    "## 4. Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZBfo0m40vB6"
   },
   "source": [
    "In last session we used fully connected network to clasify digits.\n",
    "Try to build the convolutional network: use three convolutional layers, then flatten the ouput and apply 1 fully connected.\n",
    "You can use the following helper function. Notice: there is a stride parameter. It allows to effectively downscale the feature maps.\n",
    "To get an understanding of different convolution types, check the <a href=\"https://github.com/vdumoulin/conv_arithmetic\">animations here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM-OC3L4wyTn"
   },
   "source": [
    "You can start with something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "\n",
    "....\n",
    "\n",
    "x = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "l1 = tf.keras.layers.Conv2D(16, 3, name = 'C1', kernel_initializer='he_uniform', activation='relu')(x)\n",
    "l2 = tf.keras.layers.Conv2D(32, 3, strides=2, name = 'C2', kernel_initializer='he_uniform', activation='relu')(l1)\n",
    "l3 = tf.keras.layers.Conv2D(32, 3, strides=2, name = 'C3', kernel_initializer='he_uniform', activation='relu')(l2)\n",
    "\n",
    "l3_f = tf.keras.layers.Flatten()(l3)\n",
    "\n",
    "l4 = tf.keras.layers.Dense(units=32, name='l4', activation='relu')(l3_f)\n",
    "l5 = tf.keras.layers.Dense(units=10, name='l5', activation='softmax')(l4)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(32*7*7, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # 28x28\n",
    "        x = F.relu(self.conv2(x))  # 14x14\n",
    "        x = F.relu(self.conv3(x))  # 7x7\n",
    "        x = x.view(-1, 32*7*7)  # flatten 32x7x7 -> 32*7*7\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.fc2(x)  # no activation, we will use cross-entropy loss with logits\n",
    "        return x\n",
    "\n",
    "model = Model().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train = x_train_2d[:, np.newaxis]  # we need additional channel dimension, to get 4D (BHWC) dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEnrC5c0z-Ci"
   },
   "source": [
    "Play with layer parameters. Can you get better performance than in fully connected network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPrth-Ertmck"
   },
   "source": [
    "## 5. Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check some pretrained models"
   ],
   "metadata": {
    "id": "5yzrNhX2ZPGf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#base_model = tf.keras.applications.InceptionV3(include_top=True) # ResNet50V2\n",
    "\n",
    "# load a pretrained inception v5 model from torchhub\n",
    "base_model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True)\n",
    "\n",
    "download_from_url(url='https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt', path='imagenet_classes.txt')\n",
    "\n",
    "with open('imagenet_classes.txt') as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]"
   ],
   "metadata": {
    "id": "Y6G25jyJiIVV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import netron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "netron.start(r'C:\\Users\\newor/.cache\\torch\\hub\\checkpoints\\inception_v3_google-1a9a5a14.pth', )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPzKLLPitmcr"
   },
   "source": [
    "## 6. Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJIfPj7Wtmcs"
   },
   "source": [
    "We will use one image to check model. `img_preproc` is croped to 299x299 pixels and slightly transformed to be used as imput for the model using `inception.prepare_training_img`. `inception.training_img_to_display` is then used to convert it to displayable one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sz = 299\n",
    "img_raw = plt.imread('ML3/chelsea.jpg')\n",
    "\n",
    "img_crop = img_raw.copy()[:sz, 100:100+sz]\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].imshow(img_raw)\n",
    "axs[0].grid(False)\n",
    "axs[1].imshow(img_crop)\n",
    "axs[1].grid(False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to apply same scaling to the input asa was done for training samples. This is done with a `preprocess_input` method corresponding to a model"
   ],
   "metadata": {
    "id": "DLpxv4yKS-3a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ToDo: normalization: adapt with https://pytorch.org/hub/pytorch_vision_inception_v3/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_crop.transpose([2,0,1])[np.newaxis].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs = base_model(torch.from_numpy(img_raw.transpose([2,0,1])[np.newaxis]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs = F.softmax(probs.logits, dim=1)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get top 5 predictions\n",
    "top5 = torch.topk(probs, 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indices = top5.indices\n",
    "probs = top5.values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[class_names[i] for i in indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "all_preds = tf.keras.applications.inception_v3.decode_predictions(pred, top=-1)  #resnet_v2\n",
    "plt.semilogy([prob for class_id, class_name, prob in all_preds[0]], '.');"
   ],
   "metadata": {
    "id": "tL3fFVrrTmeM"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
