{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "Tutorial_V_tf2_Transfer_learning.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "r-csPGkyt2NU",
    "6rPsRTmIt2NR",
    "RAJ6Lsurt2Ne",
    "CNbOLvQYt2Nr",
    "DIQpBhAGO-3O",
    "pwH_H7b6t2N1",
    "Wh0y_Hr4t2N5",
    "6Rc9mTd-t2OM",
    "2K71p8F0t2OS",
    "20Mi9gfd3YGY",
    "ld9YzWme3fKg",
    "tCmZOuxKAO-9",
    "1u0_qadxt2Ok",
    "azGK6ie4Q05I",
    "rc2XnLqU4SBP"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXHZ24Eyt2NN"
   },
   "source": [
    "# Tutorial V: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "Bern Winter School on Machine Learning, 2024<br>\n",
    "Prepared by Mykhailo Vladymyrov and Matthew Vowels.\n",
    "</p>\n",
    "\n",
    "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sVP5taKt2NQ"
   },
   "source": [
    "In this session we will use the pretrained Inception model to build own image classifier. We will aslo learn how to save our trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-csPGkyt2NU"
   },
   "source": [
    "## 1. Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BJDyIDLtRhE7"
   },
   "source": [
    "colab = True # set to True is using google colab"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ja1-byr4t2NV",
    "ExecuteTime": {
     "end_time": "2024-01-08T09:36:54.196306400Z",
     "start_time": "2024-01-08T09:36:53.865175200Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" \n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.hub import download_url_to_file\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rPsRTmIt2NR"
   },
   "source": [
    "### Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qVXZj9jAt2NS",
    "ExecuteTime": {
     "end_time": "2024-01-08T09:18:37.590250700Z",
     "start_time": "2024-01-08T09:17:08.937508600Z"
    }
   },
   "source": [
    "#if colab:\n",
    "path = os.path.join(os.path.abspath('.')+'material.tgz')\n",
    "url = 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz'\n",
    "# p = tf.keras.utils.get_file(path, url)\n",
    "# Download compressed file with torch utils\n",
    "\n",
    "download_url_to_file(url=url, dst=path)\n",
    "\n",
    "tar = tarfile.open(path, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77.4M/77.4M [01:22<00:00, 988kB/s] \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 9\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# p = tf.keras.utils.get_file(path, url)\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Download compressed file with torch utils\u001B[39;00m\n\u001B[0;32m      7\u001B[0m p \u001B[38;5;241m=\u001B[39m download_url_to_file(url\u001B[38;5;241m=\u001B[39murl, dst\u001B[38;5;241m=\u001B[39mpath)\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m p\u001B[38;5;241m==\u001B[39mpath\n\u001B[0;32m     10\u001B[0m tar \u001B[38;5;241m=\u001B[39m tarfile\u001B[38;5;241m.\u001B[39mopen(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr:gz\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m tar\u001B[38;5;241m.\u001B[39mextractall()\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAJ6Lsurt2Ne"
   },
   "source": [
    "## 2. Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVU2_Upzsa4a"
   },
   "source": [
    "We load first an inception model with pretrained weights, without the final classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Inception v3 model architecture from\n",
      "    `Rethinking the Inception Architecture for Computer Vision <http://arxiv.org/abs/1512.00567>`_.\n",
      "\n",
      "    .. note::\n",
      "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
      "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
      "\n",
      "    Args:\n",
      "        weights (:class:`~torchvision.models.Inception_V3_Weights`, optional): The\n",
      "            pretrained weights for the model. See\n",
      "            :class:`~torchvision.models.Inception_V3_Weights` below for\n",
      "            more details, and possible values. By default, no pre-trained\n",
      "            weights are used.\n",
      "        progress (bool, optional): If True, displays a progress bar of the\n",
      "            download to stderr. Default is True.\n",
      "        **kwargs: parameters passed to the ``torchvision.models.Inception3``\n",
      "            base class. Please refer to the `source code\n",
      "            <https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py>`_\n",
      "            for more details about this class.\n",
      "\n",
      "    .. autoclass:: torchvision.models.Inception_V3_Weights\n",
      "        :members:\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\newor/.cache\\torch\\hub\\pytorch_vision_v0.7.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.hub.help('pytorch/vision:v0.7.0', 'inception_v3'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T09:23:48.668249800Z",
     "start_time": "2024-01-08T09:23:48.578289200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_model = torch.hub.load('pytorch/vision:v0.7.0', 'inception_v3')"
   ],
   "metadata": {
    "id": "hgEPy0eWSdoT",
    "ExecuteTime": {
     "end_time": "2024-01-08T09:12:58.273309500Z",
     "start_time": "2024-01-08T09:09:12.995525300Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\newor/.cache\\torch\\hub\\pytorch_vision_v0.7.0\n",
      "D:\\development\\Anaconda\\envs\\napari_cp2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\development\\Anaconda\\envs\\napari_cp2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\newor/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n",
      "100%|██████████| 104M/104M [03:42<00:00, 489kB/s]  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['AuxLogits',\n 'Conv2d_1a_3x3',\n 'Conv2d_2a_3x3',\n 'Conv2d_2b_3x3',\n 'Conv2d_3b_1x1',\n 'Conv2d_4a_3x3',\n 'Mixed_5b',\n 'Mixed_5c',\n 'Mixed_5d',\n 'Mixed_6a',\n 'Mixed_6b',\n 'Mixed_6c',\n 'Mixed_6d',\n 'Mixed_6e',\n 'Mixed_7a',\n 'Mixed_7b',\n 'Mixed_7c',\n 'T_destination',\n '__annotations__',\n '__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattr__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_apply',\n '_backward_hooks',\n '_backward_pre_hooks',\n '_buffers',\n '_call_impl',\n '_compiled_call_impl',\n '_forward',\n '_forward_hooks',\n '_forward_hooks_always_called',\n '_forward_hooks_with_kwargs',\n '_forward_pre_hooks',\n '_forward_pre_hooks_with_kwargs',\n '_get_backward_hooks',\n '_get_backward_pre_hooks',\n '_get_name',\n '_is_full_backward_hook',\n '_load_from_state_dict',\n '_load_state_dict_post_hooks',\n '_load_state_dict_pre_hooks',\n '_maybe_warn_non_full_backward_hook',\n '_modules',\n '_named_members',\n '_non_persistent_buffers_set',\n '_parameters',\n '_register_load_state_dict_pre_hook',\n '_register_state_dict_hook',\n '_replicate_for_data_parallel',\n '_save_to_state_dict',\n '_slow_forward',\n '_state_dict_hooks',\n '_state_dict_pre_hooks',\n '_transform_input',\n '_version',\n '_wrapped_call_impl',\n 'add_module',\n 'apply',\n 'aux_logits',\n 'avgpool',\n 'bfloat16',\n 'buffers',\n 'call_super_init',\n 'children',\n 'compile',\n 'cpu',\n 'cuda',\n 'double',\n 'dropout',\n 'dump_patches',\n 'eager_outputs',\n 'eval',\n 'extra_repr',\n 'fc',\n 'float',\n 'forward',\n 'get_buffer',\n 'get_extra_state',\n 'get_parameter',\n 'get_submodule',\n 'half',\n 'ipu',\n 'load_state_dict',\n 'maxpool1',\n 'maxpool2',\n 'modules',\n 'named_buffers',\n 'named_children',\n 'named_modules',\n 'named_parameters',\n 'parameters',\n 'register_backward_hook',\n 'register_buffer',\n 'register_forward_hook',\n 'register_forward_pre_hook',\n 'register_full_backward_hook',\n 'register_full_backward_pre_hook',\n 'register_load_state_dict_post_hook',\n 'register_module',\n 'register_parameter',\n 'register_state_dict_pre_hook',\n 'requires_grad_',\n 'set_extra_state',\n 'share_memory',\n 'state_dict',\n 'to',\n 'to_empty',\n 'train',\n 'training',\n 'transform_input',\n 'type',\n 'xpu',\n 'zero_grad']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(base_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T09:18:44.539329100Z",
     "start_time": "2024-01-08T09:18:44.403545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Inception3(\n  (Conv2d_1a_3x3): BasicConv2d(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2a_3x3): BasicConv2d(\n    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_2b_3x3): BasicConv2d(\n    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv2d_3b_1x1): BasicConv2d(\n    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (Conv2d_4a_3x3): BasicConv2d(\n    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Mixed_5b): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5c): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_5d): InceptionA(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_1): BasicConv2d(\n      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch5x5_2): BasicConv2d(\n      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6a): InceptionB(\n    (branch3x3): BasicConv2d(\n      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3): BasicConv2d(\n      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6b): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6c): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6d): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_6e): InceptionC(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7dbl_5): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (AuxLogits): InceptionAux(\n    (conv0): BasicConv2d(\n      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (conv1): BasicConv2d(\n      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n  (Mixed_7a): InceptionD(\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2): BasicConv2d(\n      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_1): BasicConv2d(\n      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_2): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_3): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch7x7x3_4): BasicConv2d(\n      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7b): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (Mixed_7c): InceptionE(\n    (branch1x1): BasicConv2d(\n      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_1): BasicConv2d(\n      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3_2b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_1): BasicConv2d(\n      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_2): BasicConv2d(\n      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3a): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch3x3dbl_3b): BasicConv2d(\n      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (branch_pool): BasicConv2d(\n      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T09:26:31.375533Z",
     "start_time": "2024-01-08T09:26:31.181081900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/inspect_inception_v3')\n",
    "xs = torch.zeros(1, 3, 299, 299)\n",
    "writer.add_graph(base_model, xs)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T09:39:15.235256200Z",
     "start_time": "2024-01-08T09:38:52.256360500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs/inspect_inception_v3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T09:39:38.525262300Z",
     "start_time": "2024-01-08T09:39:24.925349400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Dka9ZCCsqqI"
   },
   "source": [
    "And build a new model using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def fc_head(in_features, n_classes):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, 64),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(64, n_classes)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T10:11:48.114631500Z",
     "start_time": "2024-01-08T10:11:47.963889Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# freeze the parameters of the base model\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace existing fc layer with our 2-layer classication head\n",
    "in_features = base_model.fc.in_features\n",
    "n_classes = 2\n",
    "base_model.fc = fc_head(in_features, n_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T10:12:37.785447700Z",
     "start_time": "2024-01-08T10:12:37.404420600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DqAt-68cRlmp"
   },
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=0.001)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNbOLvQYt2Nr"
   },
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovWr8fDDt2Ns"
   },
   "source": [
    "The Inception network is trained on natural images: thigs we see around everyday, like sky, flowers, animals, building, cars.\n",
    "It builds an hierarchy of features, to describe what it sees. \n",
    "This features can be used to train fast on different classes of objects. E.g. [here](https://www.tensorflow.org/tutorials/image_retraining) are more examples on transfer learning.\n",
    "\n",
    "Here you will see that these features can be even used to detect thngs very different from natural images. Namely we will try to use it to distinguish German text from Italian. We will use 100 samples, taken from 5 German and 5 Italian books, 10 samples each."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_training_img(img, crop=True, resize=True, img_size=(256, 256)):\n",
    "    if img.dtype != np.uint8:\n",
    "        img *= 255.0\n",
    "\n",
    "    if crop:\n",
    "        crop = np.min(img.shape[:2])\n",
    "        r = (img.shape[0] - crop) // 2\n",
    "        c = (img.shape[1] - crop) // 2\n",
    "        cropped = img[r: r + crop, c: c + crop]\n",
    "    else:\n",
    "        cropped = img\n",
    "\n",
    "    if resize:\n",
    "        img_pil = Image.fromarray(cropped)\n",
    "        img_pil = img_pil.resize(img_size, Image.ANTIALIAS)\n",
    "        resized = np.array(img_pil.convert('RGB'))\n",
    "    else:\n",
    "        resized = cropped.copy()\n",
    "\n",
    "    if resized.ndim == 2:\n",
    "        resized = resized[..., np.newaxis]\n",
    "    if resized.shape[2] == 4:\n",
    "        resized = resized[..., :3]\n",
    "    if resized.shape[2] == 1:\n",
    "        resized = np.concatenate((resized, resized, resized), axis=2)\n",
    "\n",
    "    resized = resized.astype(np.float32)\n",
    "\n",
    "    img_preproc = tf.keras.applications.inception_v3.preprocess_input(resized)\n",
    "    # subtract imagenet mean\n",
    "    return img_preproc\n",
    "\n",
    "def training_img_to_display(img):\n",
    "  return (img+1)/2"
   ],
   "metadata": {
    "id": "cYqmLWIdnrgT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc_89xtZt2Ns"
   },
   "source": [
    "text_label = ['German', 'Italian']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lOD1q75lt2Nu"
   },
   "source": [
    "labels0 = []\n",
    "images0 = []\n",
    "labels1 = []\n",
    "images1 = []\n",
    "\n",
    "#German\n",
    "for book in range(1,6):\n",
    "    for sample in range(1,11):\n",
    "        img = plt.imread('ML3/de/%d_%d.jpg'%(book, sample))\n",
    "        assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "        images0.append(prepare_training_img(img))\n",
    "        labels0.append([1,0])\n",
    "for book in range(1,6):\n",
    "    for sample in range(1,11):\n",
    "        img = plt.imread('ML3/it/%d_%d.jpg'%(book, sample))\n",
    "        assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "        images1.append(prepare_training_img(img))\n",
    "        labels1.append([0,1])\n",
    "        \n",
    "idx = np.random.permutation(len(labels0))\n",
    "labels0 = np.array(labels0)[idx]\n",
    "images0 = np.array(images0)[idx]\n",
    "labels1 = np.array(labels1)[idx]\n",
    "images1 = np.array(images1)[idx]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [],
    "id": "85HJPDcAt2Nz",
    "scrolled": false
   },
   "source": [
    "#We will take 80% from each for training and 20 for validation\n",
    "n_half = images0.shape[0]\n",
    "n_train_half = n_half*80//100\n",
    "n_train = n_train_half*2\n",
    "\n",
    "x_train = np.concatenate([images0[:n_train_half], images1[:n_train_half]])\n",
    "y_train = np.concatenate([labels0[:n_train_half], labels1[:n_train_half]])\n",
    "\n",
    "x_valid = np.concatenate([images0[n_train_half:], images1[n_train_half:]])\n",
    "y_valid = np.concatenate([labels0[n_train_half:], labels1[n_train_half:]])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2Pf2tEpt2Nw"
   },
   "source": [
    "Lets see a sample:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DUu3dTUXt2Nw"
   },
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(10,10))\n",
    "img_d = training_img_to_display(images0[25])\n",
    "axs[0].imshow(img_d)\n",
    "axs[0].grid(False)\n",
    "img_d = training_img_to_display(images1[25])\n",
    "axs[1].imshow(img_d)\n",
    "axs[1].grid(False)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIQpBhAGO-3O"
   },
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apBoSWOOt2Ny"
   },
   "source": [
    "The training is similar to what we we saw previously.\n",
    "\n",
    "Since Inception model is big, this will take a while, even we use GPUs. On your laptop CPU this would probably take ~15 times longer. And we are not training the whole Inception! We have just small thing on top + a very small dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibxBnLALPNn_"
   },
   "source": [
    "We will use callback to save checkpoints on each iteration of training. They contain values of trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H6TwMc5jaU-P",
    "scrolled": true
   },
   "source": [
    "save_path = 'save/text_{epoch}.ckpt'\n",
    "\n",
    "batch_size=10\n",
    "n_itr_per_epoch = len(x_train) // batch_size\n",
    "save_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_path,\n",
    "                                                   save_weights_only=True,\n",
    "                                                   save_freq=1 * n_itr_per_epoch) # save every 1 epochs\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 epochs=150, batch_size=batch_size, \n",
    "                 validation_data=(x_valid, y_valid),\n",
    "                 callbacks=[save_callback])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VU6aG1zlaVcA"
   },
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].plot(hist.epoch, hist.history['loss'])\n",
    "axs[0].plot(hist.epoch, hist.history['val_loss'])\n",
    "axs[0].legend(('training loss', 'validation loss'), loc='lower right')\n",
    "axs[1].plot(hist.epoch, hist.history['accuracy'])\n",
    "axs[1].plot(hist.epoch, hist.history['val_accuracy'])\n",
    "\n",
    "axs[1].legend(('training accuracy', 'validation accuracy'), loc='lower right')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr5x3x2at2N0"
   },
   "source": [
    "We see that training accuracy hits 100% quickly. Why do you think it happens? Consider that loss keeps decreasing.\n",
    "Also on such a small dataset our model overfits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwH_H7b6t2N1"
   },
   "source": [
    "## 5. Load trained variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrtttZKEt2N1"
   },
   "source": [
    "If we have the model already created we can easily load the saved training variables values from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6uc8TW0vt2N2"
   },
   "source": [
    "#in the beginning:\n",
    "model.load_weights('save/text_5.ckpt')\n",
    "model.evaluate(images1[:1],  labels1[:1], verbose=2)\n",
    "\n",
    "#in the end:\n",
    "model.load_weights('save/text_150.ckpt')\n",
    "model.evaluate(images1[:1],  labels1[:1], verbose=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh0y_Hr4t2N5"
   },
   "source": [
    "## 6. Saving for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24LEtunzQGMf"
   },
   "source": [
    "In tf2 it's easy to save a model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zKSZz5Jnt2OK"
   },
   "source": [
    "tf.saved_model.save(model, \"inference_model/\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Rc9mTd-t2OM"
   },
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cJIgyaB7t2ON"
   },
   "source": [
    "mod = tf.saved_model.load('inference_model')\n",
    "func = mod.signatures[\"serving_default\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y-qCAKnEd2p6"
   },
   "source": [
    "output_name = model.output_names[0]  # single output\n",
    "print(output_name)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nu7AI5U4t2OP"
   },
   "source": [
    "res = func(tf.constant(images1[:1]))[output_name]\n",
    "print(res)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZMfDpJwiPD0"
   },
   "source": [
    "Or we can make a nice wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "XmDxdQoDeUey"
   },
   "source": [
    "class Inferer:\n",
    "    def __init__(self, model_path, output_name):\n",
    "        self.mod = tf.saved_model.load(model_path)\n",
    "        self.func = self.mod.signatures[\"serving_default\"]\n",
    "        self.output_name = output_name\n",
    "        self.class_names = np.array(['german', 'italian'])\n",
    "        self.max_len = 64\n",
    "\n",
    "    def infere_class_batch(self, inputs):\n",
    "        probabilities = self.func(tf.constant(inputs))[self.output_name].numpy()\n",
    "        classes = np.argmax(probabilities, axis=1)\n",
    "        probs = probabilities[np.arange(len(classes)), classes]\n",
    "        return classes, probs\n",
    "\n",
    "    def infere_class(self, inputs):\n",
    "        n = len(inputs)\n",
    "        if n > self.max_len:\n",
    "            classes = []\n",
    "            probs = []\n",
    "            for i in range( (n+self.max_len-1) // self.max_len):\n",
    "                batch = inputs[i* self.max_len : (i+1)* self.max_len]\n",
    "                batch_classes, batch_probs = self.infere_class_batch(batch)\n",
    "                classes.append(batch_classes)\n",
    "                probs.append(batch_probs)\n",
    "            classes = np.concatenate(classes)\n",
    "            probs = np.concatenate(probs)\n",
    "        else:\n",
    "            classes, probs = self.infere_class_batch(inputs)\n",
    "  \n",
    "        return classes, probs\n",
    "\n",
    "    def infere(self, inputs, prob=False):\n",
    "        classes, probs = self.infere_class(inputs)\n",
    "        cn = self.class_names[classes]\n",
    "        return (cn, probs) if prob else cn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ZVASuj1jYLx"
   },
   "source": [
    "inf = Inferer('inference_model', output_name)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YJSEv3ctfmwy"
   },
   "source": [
    "inf.infere(images0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EpLPDU87lBMp"
   },
   "source": [
    "images_all = np.concatenate([images0, images1])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GOqCpeZ4fyTh"
   },
   "source": [
    "inf.infere(images_all, prob=True) # ouput class confidence probability"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K71p8F0t2OS"
   },
   "source": [
    "## 8. Improving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwVcKUelt2OU"
   },
   "source": [
    "Often, as in this sample we don't have anough labeled data in hand. We need to use it as efficient as possible.\n",
    "One way to do it is to aply training data augmentation: we can slightly distort it, e.g. rescale, to effectively multiply the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoER6gFgt2OV"
   },
   "source": [
    "We will generate rescaled images, minimum - to have smaller dimension equal 256, maximum - 130%. Let's define a function which will do this job:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lBX83LXot2OV"
   },
   "source": [
    "def get_random_scaled_img(file, minsize = 256, scalemax=1.3):\n",
    "    im = Image.open(file)\n",
    "    w, h = im.size\n",
    "    # get minimal possible size\n",
    "    scalemin =float(minsize) / min(w,h)\n",
    "    # get a rescale factor from a uniform distribution.\n",
    "    scale = scalemin + np.random.rand() * (scalemax - scalemin)\n",
    "    w1 = int(max(minsize, scale*w))\n",
    "    h1 = int(max(minsize, scale*h))\n",
    "    \n",
    "    #rescale with smoothing\n",
    "    im1 = im.resize((w1,h1), Image.ANTIALIAS)\n",
    "    #get numpy array from the PIL Image\n",
    "    img_arr = np.array(im1.convert('RGB'))\n",
    "\n",
    "    #crop to 256x256, preventing further resize by prepare_training_img\n",
    "    r = (img_arr.shape[0] - minsize) // 2\n",
    "    c = (img_arr.shape[1] - minsize) // 2\n",
    "    img_arr = img_arr[r:r+minsize,c:c+minsize]\n",
    "\n",
    "    return img_arr"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL8Qt87it2OX"
   },
   "source": [
    "Lets check rescaled images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0C4aL8x_t2OX"
   },
   "source": [
    "n_smpl=2\n",
    "scaled_imgs=[get_random_scaled_img('ML3/de/%d_%d.jpg'%(1, 1)) for i in range(n_smpl**2)]\n",
    "fig, ax = plt.subplots(n_smpl, n_smpl, figsize=(n_smpl*4, n_smpl*4))\n",
    "for row in range(n_smpl):\n",
    "    for col in range(n_smpl):\n",
    "        ax[col, row].imshow(scaled_imgs[row*n_smpl+col])\n",
    "        ax[col, row].grid(False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kisfRq3vt2OZ"
   },
   "source": [
    "Read again images, now generating 5 rescaled from each one."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o3DtE8hQt2OZ"
   },
   "source": [
    "labels0 = []\n",
    "images0 = []\n",
    "labels1 = []\n",
    "images1 = []\n",
    "\n",
    "mult = 5\n",
    "#German\n",
    "for book in range(1,6):\n",
    "    for sample in range(1,11):\n",
    "        for itr in range(mult):\n",
    "            img = get_random_scaled_img('ML3/de/%d_%d.jpg'%(book, sample))\n",
    "            assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "            images0.append(prepare_training_img(img))\n",
    "            labels0.append([1,0])\n",
    "#Italian\n",
    "for book in range(1,6):\n",
    "    for sample in range(1,11):\n",
    "        for itr in range(mult):\n",
    "            img = get_random_scaled_img('ML3/it/%d_%d.jpg'%(book, sample))\n",
    "            assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "            images1.append(prepare_training_img(img))\n",
    "            labels1.append([0,1])\n",
    "        \n",
    "idx = np.random.permutation(len(labels0))\n",
    "labels0 = np.array(labels0)[idx]\n",
    "images0 = np.array(images0)[idx]\n",
    "labels1 = np.array(labels1)[idx]\n",
    "images1 = np.array(images1)[idx]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XPTso1cft2Ob"
   },
   "source": [
    "#We will take 80% from each for training and 20 for validation\n",
    "n_half = images0.shape[0]\n",
    "n_train_half = n_half*80//100\n",
    "n_train = n_train_half*2\n",
    "\n",
    "x_train = np.concatenate([images0[:n_train_half], images1[:n_train_half]])\n",
    "y_train = np.concatenate([labels0[:n_train_half], labels1[:n_train_half]])\n",
    "\n",
    "x_valid = np.concatenate([images0[n_train_half:], images1[n_train_half:]])\n",
    "y_valid = np.concatenate([labels0[n_train_half:], labels1[n_train_half:]])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeDPdwjHt2Ob"
   },
   "source": [
    "And finally do training again, same way. Just now we change the number of epochs: before we had 150, but now that we have 5 times more training data we'll do 60. While 60 > 150/5, it looks like it takes a bit more time to converge.\n",
    "We use the same graph as before, `g2`, the one we can train."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bzum4jNQRhHB"
   },
   "source": [
    "#We will take 80% from each for training and 20 for validation\n",
    "n_half = images0.shape[0]\n",
    "n_train_half = n_half*80//100\n",
    "n_train = n_train_half*2\n",
    "\n",
    "x_train = np.concatenate([images0[:n_train_half], images1[:n_train_half]])\n",
    "y_train = np.concatenate([labels0[:n_train_half], labels1[:n_train_half]])\n",
    "\n",
    "x_valid = np.concatenate([images0[n_train_half:], images1[n_train_half:]])\n",
    "y_valid = np.concatenate([labels0[n_train_half:], labels1[n_train_half:]])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3s2lS9FLZHqw"
   },
   "source": [
    "x = tf.keras.layers.Input(shape=(256,256,3), dtype=tf.float32)\n",
    "\n",
    "base_model.trainable = False\n",
    "base_out = base_model(x)\n",
    "\n",
    "base_out_f = tf.keras.layers.GlobalAveragePooling2D()(base_out)\n",
    "\n",
    "h1 = tf.keras.layers.Dense(64, activation='sigmoid')(base_out_f)\n",
    "h2 = tf.keras.layers.Dense(2, activation='softmax')(h1)\n",
    "\n",
    "model_aug = tf.keras.Model(x, h2)\n",
    "\n",
    "model_aug.compile(optimizer=tf.keras.optimizers.Adam(0.0005,) ,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hX8D7NZ7lsEl"
   },
   "source": [
    "save_path = 'save/text_augmented_{epoch}.ckpt'\n",
    "\n",
    "batch_size=10\n",
    "n_itr_per_epoch = len(x_train) // batch_size\n",
    "save_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_path,\n",
    "                                                   save_weights_only=True,\n",
    "                                                   save_freq=1 * n_itr_per_epoch) # save every 1 epochs\n",
    "                                                   \n",
    "hist = model_aug.fit(x_train, y_train,\n",
    "                 epochs=60, batch_size=batch_size, \n",
    "                 validation_data=(x_valid, y_valid),\n",
    "                 callbacks=[save_callback])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TJlKctX6l32R"
   },
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].plot(hist.epoch, hist.history['loss'])\n",
    "axs[0].plot(hist.epoch, hist.history['val_loss'])\n",
    "axs[0].legend(('training loss', 'validation loss'), loc='lower right')\n",
    "axs[1].plot(hist.epoch, hist.history['accuracy'])\n",
    "axs[1].plot(hist.epoch, hist.history['val_accuracy'])\n",
    "\n",
    "axs[1].legend(('training accuracy', 'validation accuracy'), loc='lower right')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QsLty-0Rl_u1"
   },
   "source": [
    "# model_aug.load_weights('save/text_augmented_23.ckpt')\n",
    "tf.saved_model.save(model_aug, \"inference_model_aug/\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHZx8jQot2Oc"
   },
   "source": [
    "We had a REEEALLY small dataset for such a complicated task. Does it really generalize? mb it just memorizes all the images we fed into it? Lets perform a test. `w1.PNG` and `w2.PNG` are text screenshots from wikipedia in [Italian](https://it.wikipedia.org/wiki/Apprendimento_automatico) and [German](https://de.wikipedia.org/wiki/Maschinelles_Lernen)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XGbAqHCWt2Od"
   },
   "source": [
    "# load images\n",
    "im_wiki_1 = plt.imread('ML3/w1.jpg')\n",
    "im_wiki_2 = plt.imread('ML3/w2.jpg')\n",
    "\n",
    "# crop/covert for proper color range\n",
    "im_wiki_1_p = prepare_training_img(im_wiki_1)[np.newaxis]\n",
    "im_wiki_2_p = prepare_training_img(im_wiki_2)[np.newaxis]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4T6yv7z8mPc-"
   },
   "source": [
    "output_name = model_aug.output_names[0]\n",
    "inf = Inferer('inference_model_aug', output_name)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YCXZInp8mPdC"
   },
   "source": [
    "class_name, prob = inf.infere(np.concatenate([im_wiki_1_p, im_wiki_2_p]), prob=True)\n",
    "\n",
    "\n",
    "print('probabilities for w1:', prob[0], 'detected language:', class_name[0])\n",
    "print('probabilities for w2:', prob[1], 'detected language:', class_name[1])\n",
    "\n",
    "# Show image crops\n",
    "plt.imshow( training_img_to_display(im_wiki_1_p[0]))\n",
    "plt.show()\n",
    "plt.imshow( training_img_to_display(im_wiki_2_p[0]))\n",
    "plt.show()\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN0LvX7dt2Of"
   },
   "source": [
    "## 9. Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do in 4 groups (35 min), in the end present results (2 min/group)"
   ],
   "metadata": {
    "id": "qIylnFqtVbVD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 1."
   ],
   "metadata": {
    "id": "20Mi9gfd3YGY"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGzj4VPFt2Og"
   },
   "source": [
    "There is a serious problem in the example above: the training and validation datasets are not independent. We generated 5 randomly scaled images from each initial image. With high probability from 5 images (generated from same initial one!) some will end up im the training and some in validation datasets. Since they are generated from the same initial ones, they are not fully independent. This compromises evaluation of model performance, leading to an overestimate of the performance.\n",
    "\n",
    "1. Modify the generation of the training and validation datasets to fulfil requirenment of independance.\n",
    "2. Check how validation accuracy and loss changes\n",
    "\n",
    "Do not look solution to Option 1 ^_^"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 2."
   ],
   "metadata": {
    "id": "ld9YzWme3fKg"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiPqMhoQ3fKg"
   },
   "source": [
    "In the situation when the model is likely to overfit, final performance would especially depend on the learning rate.\n",
    "\n",
    "* Plot the best validation accuracy vs learning rate.\n",
    "* Plot the number of epochs untill the best validation accuracy vs learning rate.\n",
    "\n",
    "Test learning rates within +/- 1.5 orders of magnitude, i.e. from 30 times smaller to 30 times larger learning rates, than the current one.\n",
    "\n",
    "Get 3 replicates.\n",
    "\n",
    "Use test/validation split from solution to Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 3."
   ],
   "metadata": {
    "id": "tCmZOuxKAO-9"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgnyYP0PAbtK"
   },
   "source": [
    "In the situation when the model is likely to overfit, final performance would especially depend on the model's architecture.\n",
    "\n",
    "* Plot the best validation accuracy vs width of the first dense layer.\n",
    "* Plot the number of epochs untill the best validation accuracy vs width.\n",
    "\n",
    "Test number of features in the first dense layer between 8 and 4096.\n",
    "\n",
    "Get 3 replicates.\n",
    "\n",
    "Use test/validation split from solution to Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 4."
   ],
   "metadata": {
    "id": "kPkcdbb3CnUT"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8OGYj8eCnUU"
   },
   "source": [
    "We tested one model for the backbone of trnasfer learning. Try other ones (at least 3): https://www.tensorflow.org/api_docs/python/tf/keras/applications.\n",
    "\n",
    "\n",
    "* Plot the best validation accuracy vs model.\n",
    "* Plot the number of epochs until the best validation accuracy vs model.\n",
    "* Does any model generalize to wiki data?\n",
    "\n",
    "Run 3 replicates for each model. Input size might vary for different models, adjust the crop size accordingly in parameters to `prepare_training_img`.\n",
    "\n",
    "Use test/validation split from solution to Exercise 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u0_qadxt2Ok"
   },
   "source": [
    "## 10. Homework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more information on transfer learing look https://www.tensorflow.org/guide/keras/transfer_learning"
   ],
   "metadata": {
    "id": "MIrjlliiqWWv"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SetU7UKt2Ol"
   },
   "source": [
    "So far we scaled images as a whole.\n",
    "Which other augmentations would make sence for the text data?\n",
    "Check https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azGK6ie4Q05I"
   },
   "source": [
    "## 11. Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 1"
   ],
   "metadata": {
    "id": "rc2XnLqU4SBP"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwUWaKdeRhHe"
   },
   "source": [
    "To prevent same rescaled versions of the same image ending up in both training and validation sets, we could split the dataset first"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cPBjfz05RhHe"
   },
   "source": [
    "np.random.seed(42)  # ensure all groups and runs to have the same training/validation split.\n",
    "\n",
    "# list all samples\n",
    "de_book_sample = [(book, sample) for book in range(1,6) for sample in range(1,11)]\n",
    "it_book_sample = [(book, sample) for book in range(1,6) for sample in range(1,11)]\n",
    "\n",
    "de_book_sample = np.array(de_book_sample)\n",
    "it_book_sample = np.array(it_book_sample)\n",
    "\n",
    "# get array of permultation indexes\n",
    "n_half = len(de_book_sample)  # size of both datasets is identical\n",
    "de_idx = np.random.permutation(n_half)\n",
    "it_idx = np.random.permutation(n_half)\n",
    "\n",
    "# shuffle list of samples\n",
    "de_book_sample = de_book_sample[de_idx]\n",
    "it_book_sample = it_book_sample[it_idx]\n",
    "\n",
    "# split training and validation\n",
    "# We will take 80% from each for training and 20 for validation\n",
    "n_train_half = n_half*80//100\n",
    "n_train = n_train_half*2\n",
    "\n",
    "de_book_sample_train = de_book_sample[:n_train_half] # first 80 %\n",
    "de_book_sample_valid = de_book_sample[n_train_half:] # remaining part\n",
    "\n",
    "it_book_sample_train = it_book_sample[:n_train_half] # first 80 %\n",
    "it_book_sample_valid = it_book_sample[n_train_half:] # remaining part\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "\n",
    "mult = 10\n",
    "for itr in range(mult):\n",
    "    # each pair [book,sample] goes to either training or validation set, not both\n",
    "    # German training\n",
    "    for book, sample in de_book_sample_train:\n",
    "        img = get_random_scaled_img('ML3/de/%d_%d.jpg'%(book, sample), scalemax=1.5)\n",
    "        assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "        x_train.append(prepare_training_img(img))\n",
    "        y_train.append([1,0])\n",
    "  \n",
    "    # Italian training\n",
    "    for book, sample in it_book_sample_train:\n",
    "        img = get_random_scaled_img('ML3/it/%d_%d.jpg'%(book, sample), scalemax=1.5)\n",
    "        assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "        x_train.append(prepare_training_img(img))\n",
    "        y_train.append([0,1])\n",
    "  \n",
    "    # German validation\n",
    "    for book, sample in de_book_sample_valid:\n",
    "        img = get_random_scaled_img('ML3/de/%d_%d.jpg'%(book, sample), scalemax=1.5)\n",
    "        assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "        x_valid.append(prepare_training_img(img))\n",
    "        y_valid.append([1,0])\n",
    "  \n",
    "    # Italian validation\n",
    "    for book, sample in it_book_sample_valid:\n",
    "        img = get_random_scaled_img('ML3/it/%d_%d.jpg'%(book, sample), scalemax=1.5)\n",
    "        assert(img.shape[0]>=256 and img.shape[1]>=256 and len(img.shape)==3)\n",
    "        x_valid.append(prepare_training_img(img))\n",
    "        y_valid.append([0,1])\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_valid = np.array(x_valid)\n",
    "y_valid = np.array(y_valid)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "uKy_7H6dzY2r"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
