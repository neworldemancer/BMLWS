{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Tutorial_V_tf2_Transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r-csPGkyt2NU",
        "6rPsRTmIt2NR",
        "RAJ6Lsurt2Ne",
        "CNbOLvQYt2Nr",
        "DIQpBhAGO-3O",
        "pwH_H7b6t2N1",
        "Wh0y_Hr4t2N5",
        "6Rc9mTd-t2OM",
        "2K71p8F0t2OS",
        "20Mi9gfd3YGY",
        "ld9YzWme3fKg",
        "tCmZOuxKAO-9",
        "1u0_qadxt2Ok",
        "azGK6ie4Q05I",
        "rc2XnLqU4SBP"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXHZ24Eyt2NN"
      },
      "source": [
        "# Tutorial V: Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "Bern Winter School on Machine Learning, 2024<br>\n",
        "Prepared by Mykhailo Vladymyrov and Matthew Vowels.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ],
      "metadata": {
        "collapsed": false,
        "id": "7i1Ub7uL9Ewx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sVP5taKt2NQ"
      },
      "source": [
        "In this session we will use the pretrained Inception model to build own image classifier. We will aslo learn how to save our trained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-csPGkyt2NU"
      },
      "source": [
        "## 1. Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDyIDLtRhE7"
      },
      "source": [
        "colab = True # set to True is using google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja1-byr4t2NV",
        "ExecuteTime": {
          "end_time": "2024-01-08T13:17:01.708389400Z",
          "start_time": "2024-01-08T13:16:57.076954600Z"
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "import sys\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.hub import download_url_to_file\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T14:49:29.432791300Z",
          "start_time": "2024-01-08T14:49:29.308875700Z"
        },
        "id": "k7oNYjnf9Ew0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rPsRTmIt2NR"
      },
      "source": [
        "### Download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVXZj9jAt2NS",
        "ExecuteTime": {
          "end_time": "2024-01-08T12:13:50.730848Z",
          "start_time": "2024-01-08T12:13:01.714491400Z"
        }
      },
      "source": [
        "if colab:\n",
        "    path = os.path.join(os.path.abspath('.')+'material.tgz')\n",
        "    url = 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz'\n",
        "    # p = tf.keras.utils.get_file(path, url)\n",
        "    # Download compressed file with torch utils\n",
        "\n",
        "    download_url_to_file(url=url, dst=path)\n",
        "\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAJ6Lsurt2Ne"
      },
      "source": [
        "## 2. Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVU2_Upzsa4a"
      },
      "source": [
        "We load first an inception model with pretrained weights, without the final classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(torch.hub.help('pytorch/vision:v0.7.0', 'inception_v3'))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T12:13:50.814400700Z",
          "start_time": "2024-01-08T12:13:50.737850500Z"
        },
        "id": "DnybKqdD9Ew2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = torch.hub.load('pytorch/vision:v0.7.0', 'inception_v3')"
      ],
      "metadata": {
        "id": "hgEPy0eWSdoT",
        "ExecuteTime": {
          "end_time": "2024-01-08T14:54:46.816634300Z",
          "start_time": "2024-01-08T14:54:45.509781700Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "dir(base_model)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T12:13:52.066940700Z",
          "start_time": "2024-01-08T12:13:51.989406800Z"
        },
        "id": "xIxL-6us9Ew3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "base_model"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T12:13:52.099478900Z",
          "start_time": "2024-01-08T12:13:52.015936100Z"
        },
        "id": "qjXUXfDa9Ew3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "writer = SummaryWriter('runs/inspect_inception_v3')\n",
        "xs = torch.zeros(1, 3, 299, 299)\n",
        "writer.add_graph(base_model, xs)\n",
        "writer.close()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T09:39:15.235256200Z",
          "start_time": "2024-01-08T09:38:52.256360500Z"
        },
        "id": "EvErsMsS9Ew3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#%tensorboard --logdir=runs/inspect_inception_v3"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T09:39:38.525262300Z",
          "start_time": "2024-01-08T09:39:24.925349400Z"
        },
        "id": "3O7a6mG49Ew3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dka9ZCCsqqI"
      },
      "source": [
        "And build a new model using it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def fc_head(in_features, n_classes):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_features, 64),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(64, n_classes)\n",
        "    )"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T12:13:52.180529900Z",
          "start_time": "2024-01-08T12:13:52.064940200Z"
        },
        "id": "kNmxZWvb9Ew4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# freeze the parameters of the base model\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# replace existing fc layer with our 2-layer classication head\n",
        "in_features = base_model.fc.in_features\n",
        "n_classes = 2\n",
        "base_model.fc = fc_head(in_features, n_classes)\n",
        "model = base_model.to(device)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T14:54:53.489598500Z",
          "start_time": "2024-01-08T14:54:53.289436200Z"
        },
        "id": "lV-MVhz-9Ew4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqAt-68cRlmp",
        "ExecuteTime": {
          "end_time": "2024-01-08T14:55:10.349010300Z",
          "start_time": "2024-01-08T14:55:10.058358200Z"
        }
      },
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_predictions(logits):\n",
        "    # no need to apply softmax, as softmax is a monotonic function\n",
        "    # probs = F.softmax(logits, dim=1)\n",
        "    _, predictions = torch.max(logits, dim=1)\n",
        "    return predictions"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:50:11.803297400Z",
          "start_time": "2024-01-08T13:50:11.740340500Z"
        },
        "id": "7ZD_Opvb9Ew5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def calculate_accuracy_batch(predictions, labels):\n",
        "    n_correct = torch.sum(predictions == labels).item()\n",
        "    n_total = len(labels)\n",
        "    return n_correct, n_total"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:50:15.864948700Z",
          "start_time": "2024-01-08T13:50:15.654795400Z"
        },
        "id": "071zs6O19Ew5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbOLvQYt2Nr"
      },
      "source": [
        "## 3. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovWr8fDDt2Ns"
      },
      "source": [
        "The Inception network is trained on natural images: thigs we see around everyday, like sky, flowers, animals, building, cars.\n",
        "It builds an hierarchy of features, to describe what it sees.\n",
        "This features can be used to train fast on different classes of objects. E.g. [here](https://www.tensorflow.org/tutorials/image_retraining) are more examples on transfer learning.\n",
        "\n",
        "Here you will see that these features can be even used to detect thngs very different from natural images. Namely we will try to use it to distinguish German text from Italian. We will use 100 samples, taken from 5 German and 5 Italian books, 10 samples each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# load images from the ML3 folder into torch dataset\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder('ML3', transform=image_transform)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T14:57:06.564069800Z",
          "start_time": "2024-01-08T14:57:06.352130Z"
        },
        "id": "utvk-xq59Ew5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "text_label = dataset.classes\n",
        "print(text_label)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:27:23.843245Z",
          "start_time": "2024-01-08T13:27:23.625895400Z"
        },
        "id": "yRGMD3oR9Ew5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "for im, lbl in dataset:\n",
        "    print(lbl, im.shape, im.min(), im.max(), im.mean(), im.std())\n",
        "\n",
        "    # convert tensor to numpy array, transpose from CHW to HWC and scale to range [0, 1]\n",
        "    im_numpy = im.cpu().numpy().transpose(1,2,0)\n",
        "    im_numpy -= im_numpy.min(axis=(0,1))\n",
        "    im_numpy /= im_numpy.max(axis=(0,1))\n",
        "\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(im_numpy)\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:29:11.630578200Z",
          "start_time": "2024-01-08T13:29:11.273883700Z"
        },
        "id": "3gwPcEfj9Ew6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#We will take 80% from each for training and 20 for validation\n",
        "\n",
        "batch_size=10\n",
        "\n",
        "dataset_tra, dataset_val = torch.utils.data.random_split(dataset, [0.8, 0.2])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:27:24.662546Z",
          "start_time": "2024-01-08T13:27:24.261044800Z"
        },
        "id": "IiA2hmzZ9Ew6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is this a good way? Why?"
      ],
      "metadata": {
        "collapsed": false,
        "id": "-8viQSII9Ew6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "lbls = np.array([lbl for _, lbl in dataset_tra])\n",
        "print(f'n_class_0 = {np.sum(lbls == 0)}, n_class_1 = {np.sum(lbls == 1)}')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:27:29.932191100Z",
          "start_time": "2024-01-08T13:27:27.635497Z"
        },
        "id": "TU0HGzBr9Ew7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's better to have a balanced (strtifed) training and validation sets"
      ],
      "metadata": {
        "collapsed": false,
        "id": "9I-cP2MP9Ew7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "n_samples = len(dataset)\n",
        "lbls = np.array([lbl for _, lbl in dataset])\n",
        "\n",
        "# obtain indeces of training and validation samples\n",
        "idx_tra, idx_val = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=lbls)\n",
        "\n",
        "dataset_tra = torch.utils.data.Subset(dataset, idx_tra)\n",
        "dataset_val = torch.utils.data.Subset(dataset, idx_val)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:27:32.950270600Z",
          "start_time": "2024-01-08T13:27:29.811122200Z"
        },
        "id": "Rbk6uazi9Ew7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "lbls = np.array([lbl for _, lbl in dataset_tra])\n",
        "print(f'Training dataset: n_class_0 = {np.sum(lbls == 0)}, n_class_1 = {np.sum(lbls == 1)}')\n",
        "\n",
        "lbls = np.array([lbl for _, lbl in dataset_val])\n",
        "print(f'Validation dataset: n_class_0 = {np.sum(lbls == 0)}, n_class_1 = {np.sum(lbls == 1)}')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:27:35.318135600Z",
          "start_time": "2024-01-08T13:27:32.940752800Z"
        },
        "id": "QGgnPWLp9Ew8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create data loaders\n",
        "loader_tra = torch.utils.data.DataLoader(dataset_tra, batch_size=batch_size, shuffle=True)\n",
        "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:27:35.318135600Z",
          "start_time": "2024-01-08T13:27:35.276583500Z"
        },
        "id": "_eRVz7GW9Ew8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see a sample:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "pPARAsQH9Ew8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUu3dTUXt2Nw",
        "ExecuteTime": {
          "end_time": "2024-01-08T13:28:57.657730800Z",
          "start_time": "2024-01-08T13:28:53.779376700Z"
        }
      },
      "source": [
        "ims, lbls = next(iter(loader_tra))  # get a batch from the training dataset\n",
        "print(ims.shape, ims.min(), ims.max(), ims.mean(), ims.std())\n",
        "for im, lbl in zip(ims, lbls):\n",
        "    # convert tensor to numpy array, transpose from CHW to HWC and scale to range [0, 1]\n",
        "    im_numpy = im.cpu().numpy().transpose(1,2,0)\n",
        "    im_numpy -= im_numpy.min(axis=(0,1))\n",
        "    im_numpy /= im_numpy.max(axis=(0,1))\n",
        "\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(im_numpy)\n",
        "    plt.title(text_label[lbl])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model(ims.to(device))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T14:59:29.334850600Z",
          "start_time": "2024-01-08T14:59:25.244379600Z"
        },
        "id": "t5LncIzo9Ew9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "res = model(ims.to(device))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:35:58.167202200Z",
          "start_time": "2024-01-08T13:35:52.436704700Z"
        },
        "id": "0Vq5UGY99Ew9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "type(res)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:35:58.168202200Z",
          "start_time": "2024-01-08T13:35:58.120227700Z"
        },
        "id": "INWw5r8-9ExI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "res"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:37:39.679554600Z",
          "start_time": "2024-01-08T13:37:39.383869Z"
        },
        "id": "9899F9My9ExI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "logits = res.logits\n",
        "logits.shape"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T13:37:52.276580100Z",
          "start_time": "2024-01-08T13:37:52.084960400Z"
        },
        "id": "ot6C2nHh9ExI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIQpBhAGO-3O"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apBoSWOOt2Ny"
      },
      "source": [
        "The training is similar to what we saw previously.\n",
        "\n",
        "Since Inception model is big, this will take a while, even we use GPUs. On your laptop CPU this would probably take ~15 times longer. And we are not training the whole Inception! We have just small thing on top + a very small dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibxBnLALPNn_"
      },
      "source": [
        "We will use callback to save checkpoints on each iteration of training. They contain values of trainable variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_correct_running = 0\n",
        "    n_samples_running = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # reset gradients\n",
        "        logits = model(images).logits\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "\n",
        "        loss.backward()    # compute gradients\n",
        "        optimizer.step()   # update parameters with gradients\n",
        "\n",
        "        n_correct, n_samples = calculate_accuracy_batch(get_predictions(logits), labels)\n",
        "        n_correct_running += n_correct\n",
        "        n_samples_running += n_samples\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    n = len(train_loader)\n",
        "    return running_loss / n, n_correct_running / n_samples_running\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    n_correct_running = 0\n",
        "    n_samples_running = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(images)  # at the inference the model outputs just the logits\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            n_correct, n_samples = calculate_accuracy_batch(get_predictions(logits), labels)\n",
        "            n_correct_running += n_correct\n",
        "            n_samples_running += n_samples\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    n = len(val_loader)\n",
        "    return running_loss / n, n_correct_running / n_samples_running"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:06:21.450945200Z",
          "start_time": "2024-01-08T15:06:21.144715800Z"
        },
        "id": "Ir6c-Ojs9ExJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, n_epochs, train_loader, val_loader, save_path):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'epoch': []}\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['epoch'].append(epoch)\n",
        "\n",
        "        save_dict = {'epoch': epoch,\n",
        "                     'model_state_dict': model.state_dict(),\n",
        "                     'optimizer_state_dict': optimizer.state_dict(),\n",
        "                     'history': history,\n",
        "                     }\n",
        "        torch.save(save_dict, os.path.join(save_path, f'model_epoch_{epoch:03d}.pth'))\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{n_epochs} | '\n",
        "              f'train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f} | '\n",
        "              f'val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:06:22.477197Z",
          "start_time": "2024-01-08T15:06:22.328970400Z"
        },
        "id": "xsBQ1B3w9ExJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "    axs[0].plot(history['epoch'], history['train_loss'])\n",
        "    axs[0].plot(history['epoch'], history['val_loss'])\n",
        "    axs[0].legend(('training loss', 'validation loss'), loc='lower right')\n",
        "\n",
        "    axs[1].plot(history['epoch'], history['train_acc'])\n",
        "    axs[1].plot(history['epoch'], history['val_acc'])\n",
        "    axs[1].legend(('training accuracy', 'validation accuracy'), loc='lower right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:06:23.491673200Z",
          "start_time": "2024-01-08T15:06:23.269905900Z"
        },
        "id": "XbYBaMZh9ExK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "save_path = 'TL_1'\n",
        "\n",
        "n_epochs=70\n",
        "\n",
        "history = train_model(model, criterion, optimizer, n_epochs, loader_tra, loader_val, save_path)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:30:16.149697500Z",
          "start_time": "2024-01-08T15:06:24.395550700Z"
        },
        "id": "uy7h_4gl9ExK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:30:19.045430400Z",
          "start_time": "2024-01-08T15:30:18.832546300Z"
        },
        "id": "_7ikh8KY9ExK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that training accuracy hits 100% quickly. Why do you think it happens? Consider that loss keeps decreasing.\n",
        "Also on such a small dataset our model overfits."
      ],
      "metadata": {
        "collapsed": false,
        "id": "ID6-vaSH9ExL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwH_H7b6t2N1"
      },
      "source": [
        "## 5. Load trained variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrtttZKEt2N1"
      },
      "source": [
        "If we have the model already created we can easily load the saved training variables values from a checkpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "test_epochs = [0, 5, 11, 24, 69]\n",
        "for ep in test_epochs:\n",
        "    d = torch.load(os.path.join(save_path, f'model_epoch_{ep:03d}.pth'))\n",
        "\n",
        "    model_state_dict = d['model_state_dict']\n",
        "    model.load_state_dict(model_state_dict)\n",
        "\n",
        "    loss, acc = validate(model, loader_val, criterion)\n",
        "    print(f'Epoch {ep} | val_loss: {loss:.4f}, val_acc: {acc:.4f}')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:31:03.805049100Z",
          "start_time": "2024-01-08T15:30:42.953128300Z"
        },
        "id": "NbaBFyUm9ExM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh0y_Hr4t2N5"
      },
      "source": [
        "## 6. Saving for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24LEtunzQGMf"
      },
      "source": [
        "In pytorch it's easy to save a model for inference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKSZz5Jnt2OK",
        "ExecuteTime": {
          "end_time": "2024-01-08T15:31:09.950649700Z",
          "start_time": "2024-01-08T15:31:09.721307600Z"
        }
      },
      "source": [
        "# export the model for inference\n",
        "torch.save(model, 'inference_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rc9mTd-t2OM"
      },
      "source": [
        "## 7. Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJIgyaB7t2ON",
        "ExecuteTime": {
          "end_time": "2024-01-08T15:31:31.898951100Z",
          "start_time": "2024-01-08T15:31:31.714778Z"
        }
      },
      "source": [
        "loaded_model = torch.load('inference_model.pth')\n",
        "loaded_model = loaded_model.to(device)\n",
        "loaded_model.eval();  # set the model to inference mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# get an image:\n",
        "im = Image.open('ML3/it/1_1.jpg')\n",
        "# convert to tensor\n",
        "im_tensor = image_transform(im)\n",
        "# add batch dimension\n",
        "im_tensor = im_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "print(im_tensor.shape)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:32:32.970271800Z",
          "start_time": "2024-01-08T15:32:32.665595200Z"
        },
        "id": "2woKItoK9ExO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# get the logits and class\n",
        "with torch.no_grad():\n",
        "    logits = loaded_model(im_tensor)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    pred = get_predictions(logits)\n",
        "\n",
        "probs = probs.cpu().numpy()[0]  # only one element in the batch\n",
        "pred = pred.cpu().numpy()[0]\n",
        "print(probs[pred], text_label[pred])\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:33:15.149987Z",
          "start_time": "2024-01-08T15:33:14.742749600Z"
        },
        "id": "I_aYakLf9ExO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K71p8F0t2OS"
      },
      "source": [
        "## 8. Improving the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwVcKUelt2OU"
      },
      "source": [
        "Often, as in this sample we don't have anough labeled data in hand. We need to use it as efficient as possible.\n",
        "One way to do it is to aply training data augmentation: we can slightly distort it, e.g. rescale, to effectively multiply the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoER6gFgt2OV"
      },
      "source": [
        "We will generate rescaled images, minimum - to have smaller dimension equal 256, maximum - 130%. Let's define a function which will do this job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# load images from the ML3 folder into torch dataset\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "\n",
        "    #transforms.RandomCrop(512, pad_if_needed=True, fill=255),\n",
        "    #transforms.RandomCrop(299),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder('ML3', transform=image_transform)"
      ],
      "metadata": {
        "id": "iy172D_Z9ExQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "for im, lbl in dataset:\n",
        "    print(lbl, im.shape, im.min(), im.max(), im.mean(), im.std())\n",
        "\n",
        "    # convert tensor to numpy array, transpose from CHW to HWC and scale to range [0, 1]\n",
        "    im_numpy = im.cpu().numpy().transpose(1,2,0)\n",
        "    im_numpy -= im_numpy.min(axis=(0,1))\n",
        "    im_numpy /= im_numpy.max(axis=(0,1))\n",
        "\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(im_numpy)\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "id": "M6rJrYHn9ExQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "n_samples = len(dataset)\n",
        "lbls = np.array([lbl for _, lbl in dataset])\n",
        "\n",
        "# obtain indeces of training and validation samples\n",
        "idx_tra, idx_val = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=lbls)\n",
        "\n",
        "dataset_tra = torch.utils.data.Subset(dataset, idx_tra)\n",
        "dataset_val = torch.utils.data.Subset(dataset, idx_val)"
      ],
      "metadata": {
        "id": "zDCC4H1-9ExR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create data loaders\n",
        "loader_tra = torch.utils.data.DataLoader(dataset_tra, batch_size=batch_size, shuffle=True)\n",
        "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ZHy0x3qv9ExR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s2lS9FLZHqw"
      },
      "source": [
        "base_model = torch.hub.load('pytorch/vision:v0.7.0', 'inception_v3')\n",
        "\n",
        "# freeze the parameters of the base model\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# replace existing fc layer with our 2-layer classication head\n",
        "in_features = base_model.fc.in_features\n",
        "n_classes = 2\n",
        "base_model.fc = fc_head(in_features, n_classes)\n",
        "model_aug = base_model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model_aug.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX8D7NZ7lsEl"
      },
      "source": [
        "save_path = 'TL_2'\n",
        "\n",
        "n_epochs=200\n",
        "\n",
        "history = train_model(model_aug, criterion, optimizer, n_epochs, loader_tra, loader_val, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJlKctX6l32R"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHZx8jQot2Oc"
      },
      "source": [
        "We had a REEEALLY small dataset for such a complicated task. Does it really generalize? mb it just memorizes all the images we fed into it? Lets perform a test. `w1.PNG` and `w2.PNG` are text screenshots from wikipedia in [Italian](https://it.wikipedia.org/wiki/Apprendimento_automatico) and [German](https://de.wikipedia.org/wiki/Maschinelles_Lernen)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Exersize 1"
      ],
      "metadata": {
        "collapsed": false,
        "id": "EcTkNvfP9ExS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# load images\n",
        "im_wiki_1 = plt.imread('ML3/w1.jpg')\n",
        "im_wiki_2 = plt.imread('ML3/w2.jpg')"
      ],
      "metadata": {
        "id": "womFIAvW9ExS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualize the samples and perform inference"
      ],
      "metadata": {
        "collapsed": false,
        "id": "LdCs0nls9ExS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "..."
      ],
      "metadata": {
        "id": "BqoUwTcA9ExT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN0LvX7dt2Of"
      },
      "source": [
        "## 10. Excercises"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do in 4 groups (35 min), in the end present results (2 min/group)"
      ],
      "metadata": {
        "id": "qIylnFqtVbVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1."
      ],
      "metadata": {
        "id": "20Mi9gfd3YGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the situation when the model is likely to overfit, final performance would especially depend on the training datasets variety.\n",
        "\n",
        "* Experiment with changes in aspect ratio and scale, using, e.g.\n",
        "  `transforms.RandomResizedCrop(299, scale=(0.5, 2.0), ratio=(0.7, 1.3))` instead of the `transforms.RandomCrop`.\n",
        "* Which other augmentations would make sense for the text data?\n",
        "* Experiment and compare resulting best validation performance and number of epochs until the best validation accuracy.\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "O0Ywwesv9ExT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 2."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Ob0f5bYK9ExU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiPqMhoQ3fKg"
      },
      "source": [
        "In the situation when the model is likely to overfit, final performance would especially depend on the learning rate.\n",
        "\n",
        "* Plot the best validation accuracy vs learning rate.\n",
        "* Plot the number of epochs until the best validation accuracy vs learning rate.\n",
        "\n",
        "Test learning rates within +/- 1.5 orders of magnitude, i.e. from 30 times smaller to 30 times larger learning rates, than the current one.\n",
        "\n",
        "Get 3 replicates."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 3."
      ],
      "metadata": {
        "id": "tCmZOuxKAO-9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgnyYP0PAbtK"
      },
      "source": [
        "In the situation when the model is likely to overfit, final performance would especially depend on the model's architecture.\n",
        "\n",
        "* Plot the best validation accuracy vs width of the first dense layer.\n",
        "* Plot the number of epochs until the best validation accuracy vs width.\n",
        "\n",
        "Test number of features in the first dense layer between 8 and 4096.\n",
        "\n",
        "Get 3 replicates."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 4."
      ],
      "metadata": {
        "id": "kPkcdbb3CnUT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8OGYj8eCnUU"
      },
      "source": [
        "We tested one model for the backbone of transfer learning. Try other ones (at least 3): https://pytorch.org/vision/stable/models.html.\n",
        "\n",
        "\n",
        "* Plot the best validation accuracy vs model.\n",
        "* Plot the number of epochs until the best validation accuracy vs model.\n",
        "* Does any model generalize to wiki data?\n",
        "\n",
        "Run 3 replicates for each model. Input size might vary for different models, adjust the crop size accordingly in parameters to `prepare_training_img`.\n",
        "\n"
      ]
    }
  ]
}