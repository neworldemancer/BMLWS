{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial II: Optimization in **pytorch** and NN introduction\n",
        "\n",
        "<p>\n",
        "Bern Winter School on Machine Learning, 2024<br>\n",
        "Prepared by Mykhailo Vladymyrov and Matthew Vowels.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ],
      "metadata": {
        "id": "kis7ftBOG088"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# time to teach without exercises 50mins"
      ],
      "metadata": {
        "id": "4akgESUFGyxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load necessary libraries"
      ],
      "metadata": {
        "id": "g7YtYakwGzXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n"
      ],
      "metadata": {
        "id": "zaf_zTZNGwg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Linear fit\n",
        "\n",
        "Here we will solve optimization problem to perform linear regression. First we will generate training set of 80 data points and test set of 20, laying on a line with a random offset $$y = a_0 x + b_0 + \\delta,$$ where $\\delta$ is a random variable sampled from a uniform distribution with standard deviation equal to $s_0$.\n",
        "<!-- o.f.: 46, 48, 87, 99 | val better than train: 47, 56, 86. -->"
      ],
      "metadata": {
        "id": "ts4rUqQIHLDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a0 = 3  # 3\n",
        "b0 = 5  # 5\n",
        "s0 = 1  # 1\n",
        "\n",
        "n_all = 100\n",
        "\n",
        "np.random.seed(99)\n",
        "\n",
        "# all samples\n",
        "x_all = np.linspace(0, 10, n_all)  # 100 points\n",
        "d_all = np.random.uniform(-s0, s0, size=n_all)\n",
        "y_all = np.asarray([a0*x + b0 + d for x, d in zip(x_all, d_all)])\n",
        "\n",
        "# randomize order and get 80% for training\n",
        "idx = np.random.permutation(n_all)\n",
        "n_train = n_all * 80 // 100\n",
        "\n",
        "idx_train = idx[0:n_train]\n",
        "idx_val = idx[n_train:]\n",
        "\n",
        "x_train, y_train = x_all[idx_train], y_all[idx_train]\n",
        "\n",
        "x_val, y_val = x_all[idx_val], y_all[idx_val]\n",
        "\n",
        "plt.plot(x_train, y_train, \"or\", x_val, y_val, \"b^\")\n",
        "plt.legend(('training points', 'validation points'),  loc='upper left');"
      ],
      "metadata": {
        "id": "oWjUvWl8ow_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then define loss function as the mean of squared residuals (distance from line along $y$) for the points.\n",
        "\n",
        "We will use [stochactic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): on each iteration use only a fraction (`batch_size`) of all training set. In many cases training set is huge and cannot be fed on each iteration in principle. Also it can sometimes help the optimizer to properly explore the manifold."
      ],
      "metadata": {
        "id": "5tkZ42FXHTQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Linear, self).__init__()\n",
        "        self.a = nn.Parameter(torch.randn(1, requires_grad=True))\n",
        "        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.a * x + self.b\n",
        "\n",
        "\n",
        "def loss_f(true_y, y_predicted):\n",
        "    return F.mse_loss(y_predicted, true_y)"
      ],
      "metadata": {
        "id": "osw_3lYro8qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train)"
      ],
      "metadata": {
        "id": "UzVbYX1kYJhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Linear()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
        "\n",
        "# Define the number of epochs and batch size\n",
        "epochs = 400\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "x_train = torch.tensor(x_train)  # watch out for d-type issues with torch.tensors\n",
        "y_train = torch.tensor(y_train)\n",
        "\n",
        "x_val = torch.tensor(x_val)\n",
        "y_val = torch.tensor(y_val)\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # as opposed to model.eval()\n",
        "    batch_losses = []\n",
        "    permutation = torch.randperm(x_train.size()[0])\n",
        "\n",
        "\n",
        "    for i in range(0, x_train.size()[0], batch_size):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch_x, batch_y = x_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_x)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_f(batch_y, outputs)\n",
        "        batch_losses.append(loss.item())\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_training_loss = np.mean(batch_losses)\n",
        "    train_loss_history.append(average_training_loss)\n",
        "\n",
        "    # Validation step (optional, for monitoring performance)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val)\n",
        "        val_loss = loss_f(y_val, val_outputs)\n",
        "        val_loss_history.append(val_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PIevAwU4pOjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_val, b_val = model.a.item(), model.b.item()\n",
        "\n",
        "# Generate predictions for plotting\n",
        "end_fit_x = [x_all[0], x_all[-1]]\n",
        "end_fit_y = [a_val * x + b_val for x in end_fit_x]\n",
        "true_fn_y = [a0 * x + b0 for x in end_fit_x]  # Assuming a0 and b0 are known true values\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
        "\n",
        "# Plotting data points and model predictions\n",
        "axs[0].plot(x_train, y_train, 'ro', x_val, y_val, 'b^', end_fit_x, end_fit_y, 'g')\n",
        "axs[0].legend(('training points', 'test points', 'final fit'), loc='upper left')\n",
        "\n",
        "# Assuming training and validation loss history are stored in lists: train_loss_history and val_loss_history\n",
        "epochs = range(1, len(train_loss_history) + 1)\n",
        "\n",
        "# Plotting loss history\n",
        "axs[1].semilogy(epochs, train_loss_history, 'r')\n",
        "axs[1].semilogy(epochs, val_loss_history, 'b')\n",
        "axs[1].legend(('training loss', 'validation loss'), loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KX4SBPZRpmK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exercise 1\n",
        "\n",
        "Play with the true function parameters ```a0, b0, s0``` and the ``batch_size`` value, check how it affects the convergence.\n",
        "\n",
        "1. How change of `s0` affects convergance?\n",
        "2. When one should stop training to prevent overfitting?\n",
        "\n",
        "\n",
        "## 4. Training loop\n",
        "\n",
        "The training as we just saw is done iteratively, by adjusting the model parameters.\n",
        "\n",
        "We perform optimization several times for all traininng dataset. Going through all this dataset is refered to as 'epoch'.\n",
        "\n",
        "When we do training its usually done in two loops. In outer loop we iterate over all epochs. For each epoch we usually split the dataset into small chuncks, batches, and optimization it performed for all of those.\n",
        "\n",
        "It is important that data doesn't go to the training pipeline in same order. So the overall scheme looks like this (pseudocode):\n",
        "\n",
        "\n",
        "```\n",
        "x,y = get_training_data()\n",
        "for epoch in range(number_epochs):\n",
        "   x_shfl,y_shfl = shuffle(x,y)\n",
        "   \n",
        "   for mb_idx in range(number_minibatches_in_batch):\n",
        "       x_mb,y_mb = get_minibatch(x_shfl,y_shfl, mb_idx)\n",
        "       \n",
        "       optimize_on(data=x_mb, labels=y_mb)\n",
        "```\n",
        "\n",
        "## 5. Bulding blocks of a neural network\n",
        "\n",
        "Neural network consists of layers of neurons. Each neuron perfroms 2 operations.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/Perceptron.png\" alt=\"drawing\" width=\"30%\"/>\n",
        "\n",
        "Each neuron performs 2 operations.\n",
        "\n",
        "1. Calculate the linear transformation of the input vector $\\mathbf{x}_i$:\n",
        "$$z_i = \\mathbf{w} \\cdot \\mathbf{x}_i + b = \\sum_j {w_j x_{ij}} + b$$ where $\\mathbf{w}$ is a weight vector and $b$ - a bias, and $j$ traverses each dimension of the input vector\n",
        "2. Perform the nonlinear transformation of the result using the activation function $f$ $$y_i = f(z_i)$$\n",
        "\n",
        "In a fully connected neural network, each layer is a set of N neurons, performing different transformations of the input $\\mathbf{x}_i$ of the same layer, now producing an output **vector** $ \\mathbf{y} _i = f(\\mathbf{z}_i) = f(W\\mathbf{x}_i + \\mathbf{b})$ now with a bias vector $\\mathbf{b}$ and a * *matrix** of weights $W$.\n",
        "\n",
        "Since the output of each layer constitutes the input to the next layer, we can write for layer $l$: $$\\mathbf{x}^l_i = f^{l-1}(\\mathbf{W}^{ l-1} \\mathbf{x}^{ l-1}_i + \\mathbf{b}^{l-1})$$ where $\\mathbf{x}_i^{l=0}$ is the vector d 'network input for data point $i$.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/MLP.png\" alt=\"drawing\" width=\"50%\"/>\n",
        "\n",
        "To simplify building the network, we'll define a helper function, creating neuron layer with given number of outputs:"
      ],
      "metadata": {
        "id": "zxJJx8M3Hgra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense(nn.Module):\n",
        "    def __init__(self, input_units, output_units, activation=None):\n",
        "        \"\"\"\n",
        "        Fully connected layer implemented with explicit matrix multiplication.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_units : int\n",
        "            Number of input neurons\n",
        "        output_units : int\n",
        "            Number of output neurons\n",
        "        activation : callable or None, optional\n",
        "            Non-linear activation function (e.g., F.relu)\n",
        "        \"\"\"\n",
        "        super(Dense, self).__init__()\n",
        "        self.input_units = int(input_units)\n",
        "        self.output_units = int(output_units)\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = nn.Parameter(torch.randn(input_units, output_units))\n",
        "        self.bias = nn.Parameter(torch.randn(output_units))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input tensor\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output of the layer\n",
        "        \"\"\"\n",
        "\n",
        "        outputs = torch.matmul(x, self.weights) + self.bias\n",
        "\n",
        "        # Apply activation function if provided\n",
        "        if self.activation is not None:\n",
        "            # Check if the activation function is softmax\n",
        "            if self.activation == F.softmax:\n",
        "                # Apply softmax with specified dimension\n",
        "                outputs = self.activation(outputs, dim=1)\n",
        "            else:\n",
        "                # apply other activation functions normally\n",
        "                outputs = self.activation(outputs)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "nYFd690Tpz1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of multi-class classification, in the last layer, we can use the *softmax* transformation as a non-linear transformation. The softmax for the $j$th element of $\\mathbf{z}_i$ is $$y_{ij} = \\frac{e^{z_{ij}}}{\\sum_{k=1}^{K} e^{z_{ik}}}$$ where $K$ is the total number of classes.\n",
        "\n",
        "\n",
        "For example. before softmax $$[1.0, 2.0, 3.0, \\mathbf{4.0} ]$$ and after: $$[0.0321, 0.0871, 0.2369, \\mathbf{0.6439}]$$ (now the sum is equal to one)\n",
        "\n",
        "We can also now compare the fundamental truth which could be $$[0,0,0,\\mathbf{1}]$$ That is to say that class 4 is the good class and that the network predicted correctly.\n",
        "## 6. Bulding a neural network\n",
        "\n",
        "https://medium.com/geekculture/how-visualizations-help-make-sense-of-deep-learning-a408ab00688f"
      ],
      "metadata": {
        "id": "hXrPO4VLH5Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = 10\n",
        "n_output = 2\n",
        "num_datapoints = 1\n",
        "input_tensor = torch.randn(num_datapoints, n_input)\n",
        "dense = Dense(input_units=n_input , output_units=n_output, activation=torch.sigmoid)\n",
        "\n",
        "\n",
        "# remember to matrix multiply A and B we need the shapes (a1 x a2) (b1 x b2) to be such that a2=b1\n",
        "# the result will be (a1 x b2)\n",
        "\n",
        "print(dense.weights.shape)\n",
        "print(input_tensor.shape)\n",
        "print(torch.matmul(input_tensor, dense.weights).shape) # this is XW  e.g. (1, 10)x(10, 2) = (1 x 2)\n",
        "\n",
        "# notice that increasing the number of data points does not change the shape of W!\n",
        "# This is important - the parameter shape should not change according to the number of datapoints, only their dimenaionality"
      ],
      "metadata": {
        "id": "jXxMgKdbuSlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if we want to stack multiple dense operations together\n",
        "hidden_dim = 10\n",
        "dense1 =  Dense(input_units=n_input , output_units=hidden_dim, activation=torch.sigmoid)\n",
        "dense2 =  Dense(input_units=hidden_dim , output_units=n_output, activation=F.softmax)\n",
        "output = dense2(dense1(input_tensor))"
      ],
      "metadata": {
        "id": "hEmmUEZOu_mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# integrate everything into an overall model\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_units, hidden_units, output_units):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define a Dense layer\n",
        "        self.dense1 = Dense(input_units, hidden_units, activation=F.relu)\n",
        "        # Define another Dense layer\n",
        "        self.dense2 = Dense(hidden_units, output_units, activation=F.softmax)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Forward pass and get predictions\n",
        "        preds = self.dense2(self.dense1(x))\n",
        "        print(preds)  # to check values before argmax\n",
        "        return torch.argmax(preds, dim=1)\n",
        "\n",
        "# Example usage\n",
        "input_units = 10\n",
        "hidden_units = 5\n",
        "output_units = 2\n",
        "model = MyModel(input_units, hidden_units, output_units)\n",
        "\n",
        "# Create a sample input tensor\n",
        "input_tensor = torch.randn(5, input_units)\n",
        "\n",
        "# Get predictions\n",
        "predictions = model.predict(input_tensor)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "8GvQqpUUvtST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "INB2Uge9yBJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uELjOPspyOtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}