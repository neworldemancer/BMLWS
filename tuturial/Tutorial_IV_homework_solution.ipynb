{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Tutorial_III.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K2DojogoQv4l",
        "VaMSPkKJtkSt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxDZW841tkSi"
      },
      "source": [
        "# Tutorial III: Handwritten digit recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nha4NG_tkSl"
      },
      "source": [
        "<p>\n",
        "Bern Winter School on Machine Learning, 2021<br>\n",
        "Prepared by Mykhailo Vladymyrov.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjuW0SKStkSm"
      },
      "source": [
        "In this session we will create a fully-connected neural network to perform handwritten digit recognition using TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dit404ghtkSr"
      },
      "source": [
        "## 1. Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E7_B0MiQv4a"
      },
      "source": [
        "colab = True # set to True is using google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fcfiHcmuAPA"
      },
      "source": [
        "if colab:\n",
        "    %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbRY_dIutkSr"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipyd\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras.datasets.mnist as mnist\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# We'll tell matplotlib to inline any drawn figures like so:\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"\"\"<style> .rendered_html code { \n",
        "    padding: 2px 5px;\n",
        "    color: #0000aa;\n",
        "    background-color: #cccccc;\n",
        "} </style>\"\"\")\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2DojogoQv4l"
      },
      "source": [
        "### Download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZokqZcQWgCj0"
      },
      "source": [
        "if colab:\n",
        "    p = tf.keras.utils.get_file('./material.tgz', 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz')\n",
        "    !mv {p} .\n",
        "    !tar -xvzf material.tgz > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8RkDsYJQv4o"
      },
      "source": [
        "from utils import gr_disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohH_OvIXQv4r"
      },
      "source": [
        "def show_graph(g=None, gd=None):\n",
        "    gr_disp.show_graph(g, gd)\n",
        "    %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaMSPkKJtkSt"
      },
      "source": [
        "### 1.1. A bit of things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcPIsppKtkSu"
      },
      "source": [
        "The training as we saw in in last section is done iteratively, adjusting the model parameters on each step.\n",
        "\n",
        "We perform optimization several times for all traininng dataset. Going through all this dataset is refered to as 'epoch'.\n",
        "\n",
        "When we do training its usually done in two loops. In outer loop we iterate over all epochs. For each epoch we usually split the dataset into small chuncks, 'mini-batches'. Optimization is then performed on each minibatch.\n",
        "\n",
        "It is important that data doesn't go to the training pipeline in same order. So the overall scheme looks like this (pseudocode):\n",
        "\n",
        "\n",
        "```\n",
        "x,y = get_training_data()\n",
        "for epoch in range(number_epochs):\n",
        "   x_shfl,y_shfl = shuffle(x,y)\n",
        "   \n",
        "   for mb_idx in range(number_minibatches_in_batch):\n",
        "       x_mb,y_mb = get_minibatch(x_shfl,y_shfl, mb_idx)\n",
        "       \n",
        "       optimize_on(data=x_mb, labels=y_mb)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTwTRW92tkSu"
      },
      "source": [
        "Shuffling can be easily done using permuted indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y5IoSaEtkSv"
      },
      "source": [
        "#some array\n",
        "arr = np.array([110,111,112,113,114,115,116])\n",
        "\n",
        "#we can get sub-array for a set of indexes, eg:\n",
        "idx_1_3 = [1,3]\n",
        "sub_arr_1_3 = arr[idx_1_3]\n",
        "print (arr,'[',idx_1_3,']','->', sub_arr_1_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdnYF8lQtkSw"
      },
      "source": [
        "ordered_idx = np.arange(7)\n",
        "permuteded_idx = np.random.permutation(7)\n",
        "print(ordered_idx)\n",
        "print(permuteded_idx)\n",
        "\n",
        "permuted_arr = arr[permuteded_idx]\n",
        "print (arr,'[',permuteded_idx,']','->', permuted_arr)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GA-bFOMtkSy"
      },
      "source": [
        "Some additional np things in this seminar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhimaptitkSz"
      },
      "source": [
        "#index of element with highest value\n",
        "np.argmax(permuted_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb0wl9mFtkS0"
      },
      "source": [
        "arr2d = np.array([[0,1],[2,3]])\n",
        "print(arr2d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6OOEX3BtkS2"
      },
      "source": [
        "#flatten\n",
        "arr_flat = arr2d.flatten()\n",
        "#reshape\n",
        "arr_4 = arr2d.reshape((4))\n",
        "arr_4_1 = arr2d.reshape((4,1))\n",
        "\n",
        "print (arr_flat)\n",
        "print (arr_4)\n",
        "print (arr_4_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcuT4JYRtkS4"
      },
      "source": [
        "## 2. Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrbUAUlrtkS4"
      },
      "source": [
        "First we will load the data: 60000 training images and 10000 images for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L33vRL03tkS7"
      },
      "source": [
        "(x_train_2d, y_train), (x_test_2d, y_test) = mnist.load_data()\n",
        "x_train_2d = x_train_2d/255.0\n",
        "x_test_2d = x_test_2d/255.0\n",
        "\n",
        "\n",
        "print ('train: data shape', x_train_2d.shape, 'label shape', y_train.shape)\n",
        "print ('test: data shape', x_test_2d.shape, 'label shape', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1eXOuZOtkS8"
      },
      "source": [
        "Each image is a 28x28 pixels. For this model we will interppret it as a 1D array of 784 elements.\n",
        "Additionally we will use the labels a in so-called one hot encoding: each label is a vector of length 10, with all elements equal to 0, except, corresponding to the number written in the image. Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JouvebGFSzj"
      },
      "source": [
        "n_train = x_train_2d.shape[0]\n",
        "n_test = x_test_2d.shape[0]\n",
        "\n",
        "x_train = x_train_2d[..., np.newaxis]\n",
        "x_test = x_test_2d[..., np.newaxis]\n",
        "\n",
        "y_train_1_hot = np.zeros((n_train, y_train.max()+1))\n",
        "y_train_1_hot[np.arange(n_train),y_train] = 1\n",
        "\n",
        "y_test_1_hot = np.zeros((n_test, y_test.max()+1))\n",
        "y_test_1_hot[np.arange(n_test), y_test] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRO6zv6atkS_"
      },
      "source": [
        "img = x_train[0,:,:,0]\n",
        "lbl = y_train[0]\n",
        "lbl_1_hot = y_train_1_hot[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBbmxF94p8Ve"
      },
      "source": [
        "Let's check the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrN9qvs3tkTA"
      },
      "source": [
        "plt.imshow(img, cmap='gray', interpolation='nearest')\n",
        "plt.grid(False)\n",
        "print('one-hot label:',lbl_1_hot, '. Actual label:', lbl )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d-oTmh3tkTE"
      },
      "source": [
        "fig, axs = plt.subplots(5, 5, figsize=(10,10))\n",
        "for idx, im in enumerate(x_train[0:25,:,:,0]):\n",
        "    y_idx = idx // 5\n",
        "    x_idx = idx % 5\n",
        "    axs[y_idx][x_idx].imshow(im, cmap='gray', interpolation='nearest')\n",
        "    axs[y_idx][x_idx].grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzpETsFotkTF"
      },
      "source": [
        "## 3. Bulding blocks of a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Y2c3l8tkTG"
      },
      "source": [
        "Neural network consists of layers of neurons. Each neuron perfroms 2 operations.\n",
        "1. Calculate the linear transformation of the input vector $\\bar x$: \n",
        "$$z = \\bar W \\cdot \\bar x + b = \\sum {W_i x_i} + b$$ where $\\bar W$ is vector of weights and $b$ - bias.\n",
        "2. Perform the nonlinear transformation of the result using activation function $f$ $$y = f(z)$$ Here we will use rectified linear unit activation in all but last layers.\n",
        "\n",
        "In a fully connected neural network each layer is a set of N neurons, performing different transformations of all the same layer's inputs $\\bar x = [x_i]$ producing output vector $\\bar y = [y_j]_{i=1..N}$: $$y_j = f(\\bar W_j \\cdot \\bar x + b_j)$$\n",
        "\n",
        "Since output of each layer forms input of next layer, one can write for layer $l$: $$x^l_j = f(\\bar W^l_j \\cdot \\bar x^{l-1} + b^l_j)$$ where $\\bar x^0$ is network's input vactor.\n",
        "\n",
        "To simplify building the network, we'll define a helper function, creating neuron layer with given number of outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "_-6EYYNctkTG"
      },
      "source": [
        "def fully_connected_layer(x, n_output, name=None, activation=None):\n",
        "    \"\"\"Fully connected layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : tf.Tensor\n",
        "        Input tensor to connect\n",
        "    n_output : int\n",
        "        Number of output neurons\n",
        "    name : None, optional\n",
        "        TF Scope to apply\n",
        "    activation : None, optional\n",
        "        Non-linear activation function\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    h, W : tf.Tensor, tf.Tensor\n",
        "        Output of the fully connected layer and the weight matrix\n",
        "    \"\"\"\n",
        "    if len(x.get_shape()) != 2:\n",
        "        x = tf.flatten(x, reuse=None)\n",
        "\n",
        "    n_input = x.get_shape().as_list()[1]\n",
        "\n",
        "    with tf.variable_scope(name or \"fc\", reuse=None):\n",
        "        W = tf.get_variable(\n",
        "            name='W',\n",
        "            shape=[n_input, n_output],\n",
        "            dtype=tf.float32,\n",
        "            initializer=tf.initializers.glorot_uniform())\n",
        "\n",
        "        b = tf.get_variable(\n",
        "            name='b',\n",
        "            shape=[n_output],\n",
        "            dtype=tf.float32,\n",
        "            initializer=tf.initializers.constant(0.0))\n",
        "\n",
        "        h = tf.nn.bias_add(\n",
        "            name='h',\n",
        "            value=tf.matmul(x, W),\n",
        "            bias=b)\n",
        "\n",
        "        if activation:\n",
        "            h = activation(h)\n",
        "\n",
        "        return h, W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-AdR6v6_BUn"
      },
      "source": [
        "def conv_2D(x, n_output_ch,\r\n",
        "            k=3,\r\n",
        "            s=1,\r\n",
        "            activation=tf.nn.relu,\r\n",
        "            padding='VALID', name='conv2d', reuse=None\r\n",
        "           ):\r\n",
        "    \"\"\"\r\n",
        "    Helper for creating a 2d convolution operation.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        x (tf.Tensor): Input tensor to convolve.\r\n",
        "        n_output_ch (int): Number of filters.\r\n",
        "        k (int): Kernel width and height\r\n",
        "        s (int): Stride in x and y\r\n",
        "        activation (tf.Function): activation function to apply to the convolved data\r\n",
        "        padding (str): Padding type: 'SAME' or 'VALID'\r\n",
        "        name (str): Variable scope\r\n",
        "        reuse (tf.Flag): Flag whether to use existing variable. Can be False(None), True, or tf.AUTO_REUSE\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        op (tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor): Output of activation, convolution, weights, bias\r\n",
        "    \"\"\"\r\n",
        "    with tf.variable_scope(name or 'conv2d', reuse=reuse):\r\n",
        "        w = tf.get_variable(name='W',\r\n",
        "                            shape=[k, k, x.get_shape()[-1], n_output_ch],\r\n",
        "                            initializer=tf.initializers.he_uniform()\r\n",
        "                           )\r\n",
        "        \r\n",
        "        wx = tf.nn.conv2d(name='conv',\r\n",
        "                          input=x, filter=w,\r\n",
        "                          strides=[1, s, s, 1],\r\n",
        "                          padding=padding\r\n",
        "                         )\r\n",
        "        \r\n",
        "        b = tf.get_variable(name='b',\r\n",
        "                            shape=[n_output_ch], initializer=tf.initializers.constant(value=0.0)\r\n",
        "                           )\r\n",
        "        h = tf.nn.bias_add(name='h',\r\n",
        "                           value=wx,\r\n",
        "                           bias=b\r\n",
        "                          )\r\n",
        "\r\n",
        "        if activation is not None:\r\n",
        "            x = activation(h, name=activation.__name__)\r\n",
        "        else:\r\n",
        "            x = h\r\n",
        "    \r\n",
        "    return x, w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq4u6F41tkTI"
      },
      "source": [
        "In the case of classification, in the the last layer we use *softmax* transformation as non-linear transformation: $$y_i = \\sigma(\\bar z)_i = \\frac{ e^{z_i}}{\\sum_j e^{z_j}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLUWcu7-tkTJ"
      },
      "source": [
        "This will correspond to the one-hot labels that we use.\n",
        "Finally we will use the cross entropy between output $y$ and the ground truth (GT) $y_{GT}$ as the loss function: $$H(y, y_{GT}) = - \\sum_i y_{GT, i} \\log(y_{i})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agJ3gkg0tkTJ"
      },
      "source": [
        "## 4. Bulding a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfJEnLTmtkTK"
      },
      "source": [
        "Number of inputs for neurons will be given by input data, i.e. image, size. Output - by number of classes, 10 in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC1SpGLbtkTL"
      },
      "source": [
        "h, w, c = x_train.shape[1:]\n",
        "n_output = y_train_1_hot.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7SF4JZgtkTP"
      },
      "source": [
        "Let's first build a most simple network, with just 1 layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlVOlJ6VtkTP"
      },
      "source": [
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "    X = tf.placeholder(name='X', dtype=tf.float32, shape=[None, h, w, c])\n",
        "    Y = tf.placeholder(name='Y', dtype=tf.float32, shape=[None, n_output])\n",
        "    \n",
        "    L1, W1 = conv_2D(X, 16, name = 'C1')\n",
        "    L2, W2 = conv_2D(L1, 32, s=2, name = 'C2')\n",
        "    L3, W3 = conv_2D(L2, 32, s=2, name = 'C3')\n",
        "\n",
        "    L3_f = tf.keras.layers.Flatten()(L3)\n",
        "\n",
        "    L4, W4 = fully_connected_layer(L3_f , 32, 'F1', activation=tf.nn.relu)\n",
        "    L5, W5 = fully_connected_layer(L4 , 10, 'F2')\n",
        "\n",
        "    Y_onehot = tf.nn.softmax(L5, name='Logits')\n",
        "    \n",
        "    # prediction: onehot->integer\n",
        "    Y_pred = tf.argmax(Y_onehot, axis=1, name='YPred')\n",
        "    \n",
        "    # cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_onehot), reduction_indices=[1]))\n",
        "    # here we use numerically stable implementation of the same:\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=L5, labels=Y)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cross_entropy)\n",
        "    \n",
        "    # get fraction of correctly assigned labels\n",
        "    Y_true = tf.argmax(Y, 1)\n",
        "    Correct = tf.equal(Y_true, Y_pred, name='CorrectY')\n",
        "    Accuracy = tf.reduce_mean(tf.cast(Correct, dtype=tf.float32), name='Accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HIikZnSJhzT"
      },
      "source": [
        "We will train for 5 epochs, with minibatches of size 64. This is very similar to what we did in last session: split all data in minibatches, run the optimizer, for each epoch store the training and validadtion  accuracy for plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "KP4ve-rDtkTS"
      },
      "source": [
        "with tf.Session(graph=g) as sess:\n",
        "    acc_val = []\n",
        "    acc_trn = []\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Now actually do some training:\n",
        "    mini_batch_size = 64\n",
        "    n_epochs = 15\n",
        "\n",
        "    for epoch_i in range(n_epochs):\n",
        "        # iterate minibatches\n",
        "        idx = np.random.permutation(n_train)\n",
        "        for mb_idx in range(n_train // mini_batch_size):\n",
        "            sub_idx = idx[mini_batch_size * mb_idx : mini_batch_size * (mb_idx+1)]\n",
        "            x_batch, y_batch = x_train[sub_idx], y_train_1_hot[sub_idx]\n",
        "\n",
        "            sess.run(optimizer, feed_dict={\n",
        "                X: x_batch,\n",
        "                Y: y_batch\n",
        "            })\n",
        "            \n",
        "\n",
        "        acr_v = sess.run(Accuracy,\n",
        "                         feed_dict={\n",
        "                             X: x_test,\n",
        "                             Y: y_test_1_hot\n",
        "                             })\n",
        "        acr_t = sess.run(Accuracy,\n",
        "                         feed_dict={\n",
        "                             X: x_train,\n",
        "                             Y: y_train_1_hot\n",
        "                             })\n",
        "        print(acr_t, acr_v)\n",
        "        \n",
        "        acc_val.append(acr_v)\n",
        "        acc_trn.append(acr_t)\n",
        "\n",
        "    # final test accuracy:\n",
        "    corr, accr = sess.run((Correct, Accuracy),\n",
        "                          feed_dict={\n",
        "                              X: x_test,\n",
        "                              Y: y_test_1_hot\n",
        "                              })\n",
        "    \n",
        "    \n",
        "    # get index of first incorrectly recognize digit and display it\n",
        "    wrong_idx  = [i for i,c in enumerate(corr) if c == False]\n",
        "    wrong_idx0 = wrong_idx[0]\n",
        "    wrong0_lbl, L1_res, L2_res, L3_res = sess.run([Y_pred, L1, L2, L3],\n",
        "                   feed_dict={\n",
        "                       X: x_test[wrong_idx0:wrong_idx0+1],\n",
        "                       Y: y_test_1_hot[wrong_idx0:wrong_idx0+1]\n",
        "                   })\n",
        "    wrong0_lbl, L1_res, L2_res, L3_res = wrong0_lbl[0], L1_res[0], L2_res[0], L3_res[0]\n",
        "    #store final value of the W1\n",
        "    W1_res = sess.run(W1)\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "    axs[0].plot(acc_trn)\n",
        "    axs[0].plot(acc_val)\n",
        "    axs[0].legend(('training accuracy', 'validation accuracy'), loc='lower right')\n",
        "    axs[1].imshow(x_test_2d[wrong_idx0], cmap='gray', interpolation='nearest')\n",
        "    axs[1].grid(False)\n",
        "    plt.show()\n",
        "    print('found label:',wrong0_lbl, 'true label:', y_test[wrong_idx0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYoROCLrBQW"
      },
      "source": [
        "The learned model parameters W1 are a matrix of weights that show importance of each input pixel (784) for each of the 10 outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-nnCph8rU01"
      },
      "source": [
        "print(W1_res.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i02IbJ2gtkTT"
      },
      "source": [
        "Let's visualize the trained weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L371n9COtkTU"
      },
      "source": [
        "_, axs = plt.subplots(1, 16, figsize=(13,5))\n",
        "for i in range(16):\n",
        "    axs[i].imshow(W1_res[..., 0, i], cmap='gray', interpolation='nearest')\n",
        "    axs[i].grid(False)\n",
        "    axs[i].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5crzY-UrDDJX"
      },
      "source": [
        "And we can look at the extracted feature maps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XYd3ifLBucc"
      },
      "source": [
        "for L in [L1_res, L2_res, L3_res]:\r\n",
        "  n_out = L.shape[-1]\r\n",
        "\r\n",
        "  s = 5\r\n",
        "  _, axs = plt.subplots(1, n_out, figsize=(s*n_out,s))\r\n",
        "  for i in range(n_out):\r\n",
        "      axs[i].imshow(L[..., i], cmap='gray', interpolation='nearest')\r\n",
        "      axs[i].grid(False)\r\n",
        "      axs[i].axis('off')\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}