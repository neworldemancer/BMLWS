{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Tutorial_VI.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w7FnR5Kwtsq9",
        "TvQjWvbWtsrH",
        "4aqxQ_X-tsrV",
        "PCKy-0EotsrX",
        "y7-p8ClctsrY"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbpit0_rtspv"
      },
      "source": [
        "# Tutorial VI: Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Fr8e3Ltspx"
      },
      "source": [
        "<p>\n",
        "Bern Winter School on Machine Learning, 2021<br>\n",
        "Prepared by Mykhailo Vladymyrov.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Upuaj8tspy"
      },
      "source": [
        "In this session we will see what RNN is. We will use it to predict/generate text sequence, but same approach can be applied to any sequential data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QUO3V3Stspz"
      },
      "source": [
        "So far we looked at the data available altogether. In many cases the data is sequential (weather, speach, sensor signals etc).\n",
        "RNNs are specifically designed for such tasks.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/rnn.png\" alt=\"drawing\" width=\"90%\"/><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGyY2JPXtsq7"
      },
      "source": [
        "## 1. Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jKCF9MpKWdF"
      },
      "source": [
        "colab = True # set to True is using google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7erfjCJoQrc"
      },
      "source": [
        "if colab:\r\n",
        "    %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpRETZFNtsq7"
      },
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipyd\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import collections\n",
        "import time\n",
        "\n",
        "# We'll tell matplotlib to inline any drawn figures like so:\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "%load_ext tensorboard\n",
        "\n",
        "HTML(\"\"\"<style> .rendered_html code { \n",
        "    padding: 2px 5px;\n",
        "    color: #0000aa;\n",
        "    background-color: #cccccc;\n",
        "} </style>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH0SSbAftsq1"
      },
      "source": [
        "### Download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grv04xmitsq2"
      },
      "source": [
        "if colab:\n",
        "    p = tf.keras.utils.get_file('./material.tgz', 'https://scits-training.unibe.ch/data/tut_files/tpub0320.tgz')\n",
        "    !mv {p} .\n",
        "    !tar -xvzf material.tgz > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWt72gnKj4M"
      },
      "source": [
        "from utils import gr_disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqj0TR2_o_ug"
      },
      "source": [
        "def show_graph(g=None, gd=None):\r\n",
        "    gr_disp.show_graph(g, gd)\r\n",
        "    %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7FnR5Kwtsq9"
      },
      "source": [
        "## 2. Load the text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3cmvKeatsq-"
      },
      "source": [
        "def read_data(fname):\n",
        "    with open(fname) as f:\n",
        "        content = f.readlines()\n",
        "    content = [x.strip() for x in content]\n",
        "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
        "    content = np.array(content)\n",
        "    return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4KJQxKqtsrA"
      },
      "source": [
        "training_file = 'RNN/rnn.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMumjsH8tsrD"
      },
      "source": [
        "training_data = read_data(training_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GMc64ptsrF"
      },
      "source": [
        "print(training_data[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvQjWvbWtsrH"
      },
      "source": [
        "## 3. Build dataset\n",
        "We will assign an id to each word, and make dictionaries word->id and id->word.\n",
        "The most frequently repeating words have lowest id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqWfeze4tsrI"
      },
      "source": [
        "def build_dataset(words):\n",
        "    count = collections.Counter(words).most_common()\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reverse_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxjTb5VUtsrK"
      },
      "source": [
        "dictionary, reverse_dictionary = build_dataset(training_data)\n",
        "vocab_size = len(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJt3d4lJtsrL"
      },
      "source": [
        "print(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1jtdvAztsrN"
      },
      "source": [
        "Then the whole text will look as a sequence of word ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNVe-0P_tsrO"
      },
      "source": [
        "print([dictionary[w] for w in training_data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHSCevnwtsrQ"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          29
        ],
        "id": "QletTCiPtsrQ"
      },
      "source": [
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 100000\n",
        "display_step = 3000\n",
        "n_input = 10\n",
        "\n",
        "# number of units in RNN cells\n",
        "n_hidden = [256, 1024, 512, 128]\n",
        "\n",
        "def RNN(x, n_vocab, n_hid):\n",
        "\n",
        "    for n in n_hid:\n",
        "      l = tf.keras.layers.CuDNNLSTM(n, return_sequences=True, name='lstm%d' % n)  # on CPU use LSTM, on GPU use CuDNNLSTM\n",
        "      x = l(x)\n",
        "      \n",
        "\n",
        "    # there are n_input outputs but\n",
        "    # we only want the last output\n",
        "    last_output = x[:, -1]\n",
        "    \n",
        "    w = tf.Variable(tf.random_normal([n_hid[-1], n_vocab]))\n",
        "    b = tf.Variable(tf.random_normal([n_vocab]))\n",
        "    y = tf.matmul(last_output, w) + b\n",
        "    return y\n",
        "\n",
        "                    \n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "    # tf Graph input\n",
        "    x = tf.placeholder(\"float\", [None, n_input, 1])\n",
        "    y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "    \n",
        "    pred = RNN(x, vocab_size, n_hidden)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "    # Model evaluation\n",
        "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEFIMEgntsrT"
      },
      "source": [
        "show_graph(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aqxQ_X-tsrV"
      },
      "source": [
        "## 5. Run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "_iX7hcrFtsrW"
      },
      "source": [
        "with tf.Session(graph=g) as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    step = 0\n",
        "    offset = np.random.randint(0,n_input+1)\n",
        "    end_offset = n_input + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = np.random.randint(0, n_input+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
        "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
        "            symbols_out = training_data[offset + n_input]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
        "        step += 1\n",
        "        offset += (n_input+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", time.time() - start_time)\n",
        "\n",
        "    \n",
        "    for itr in range(100):\n",
        "        prompt = \"%s words: \" % n_input\n",
        "        sentence = input(prompt)\n",
        "        sentence = sentence.strip()\n",
        "        words = sentence.split(' ')\n",
        "        if len(words) != n_input:\n",
        "            continue\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            for i in range(128):\n",
        "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
        "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "                symbols_in_keys = symbols_in_keys[1:]\n",
        "                symbols_in_keys.append(onehot_pred_index)\n",
        "            print(sentence)\n",
        "        except:\n",
        "            print(\"Word not in dictionary\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCKy-0EotsrX"
      },
      "source": [
        "## 6. Excercice \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYXY5jfMAi6x"
      },
      "source": [
        "* Run with 5-7 input words instead of 3.\n",
        "* increase number of training iterations, since convergance will take much longer (training as well!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7-p8ClctsrY"
      },
      "source": [
        "## 7. Further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjhwEgdKtsrZ"
      },
      "source": [
        "[Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
        "\n",
        "[Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)"
      ]
    }
  ]
}